
[{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/tags/data-processing/","section":"Tags","summary":"","title":"Data-Processing","type":"tags"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/","section":"Hans Website","summary":"","title":"Hans Website","type":"page"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/categories/programming/","section":"Categories","summary":"","title":"Programming","type":"categories"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/tags/r/","section":"Tags","summary":"","title":"R","type":"tags"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"12 July 2024","externalUrl":null,"permalink":"/authors/%E6%96%BD%E6%89%BF%E7%BF%B0/","section":"Authors","summary":"","title":"施承翰","type":"authors"},{"content":" 遇到的問題 # 最近我在透過 R 寫入 Excel 數據時，由於資料中意外含有系統保留字元時會導致檔案無法正常讀取，造成檔案損毀的發生，因此衍生出了這篇文章，在這裡我們透過篩選出造成異常狀況的字元並移除它來避免這一結果。\n如要複現錯誤訊息可以透過下面的程式碼產生與我遇到一樣問題的 Excel：\nlibrary(openxlsx) write.xlsx(\u0026#34;\\uffff\u0026#34;, \u0026#34;test.xlsx\u0026#34;) 但須注意該字串本身無法被作為變量命名至變數中，但可以透過讀取 csv 等檔案來寫入。\n程式碼解說 # 接著我為了解決問題而編寫了下面的程式碼，透過下面的程式可以將輸入的字串轉換為沒有問題的字串。\n程式碼的運作邏輯大致可以拆解成下面的步驟：\n定義要剔除範圍的字元，並將他們編寫成一個檢索函數\n初始化一個空的字符向量用於儲存後續處理的字串\n透過逐個字符轉換為字符向量來檢查字串，並將不需剔除的字元保留下來\n將被轉換的字符向量轉換為修正後的字符串並回傳\nremove_reserved_characters \u0026lt;- function(input_string) { is_reserved \u0026lt;- function(codepoint) { return( (codepoint \u0026gt;= 0x0001 \u0026amp;\u0026amp; codepoint \u0026lt;= 0x001F) || (codepoint == 0x007F) || (codepoint \u0026gt;= 0x0080 \u0026amp;\u0026amp; codepoint \u0026lt;= 0x009F) || (codepoint \u0026gt;= 0xD800 \u0026amp;\u0026amp; codepoint \u0026lt;= 0xDFFF) || (codepoint \u0026gt;= 0xFDD0 \u0026amp;\u0026amp; codepoint \u0026lt;= 0xFDEF) || (codepoint \u0026gt;= 0xE000 \u0026amp;\u0026amp; codepoint \u0026lt;= 0xF8FF) || (codepoint \u0026gt;= 0xF0000 \u0026amp;\u0026amp; codepoint \u0026lt;= 0xFFFFD) || (codepoint \u0026gt;= 0x100000 \u0026amp;\u0026amp; codepoint \u0026lt;= 0x10FFFD) || (codepoint == 0xFFFF) ) } result \u0026lt;- integer() chars \u0026lt;- strsplit(input_string, NULL)[[1]] valid_chars \u0026lt;- sapply(chars, function(char) { codepoint \u0026lt;- utf8ToInt(char) if (length(codepoint) == 1 \u0026amp;\u0026amp; !is.na(codepoint) \u0026amp;\u0026amp; !is_reserved(codepoint)) { return(char) } else { return(NA) } }) cleaned_string \u0026lt;- paste(valid_chars[!is.na(valid_chars)], collapse = \u0026#34;\u0026#34;) return(cleaned_string) } 程式碼應用 # 在宣告上面的函數後，我們可以透過下面的程式碼來清除 data.frame 中目標欄位的問題字元：\nd \u0026lt;- read.csv(\u0026#34;/Users/hans/Downloads/textcsvdata.csv\u0026#34;) test \u0026lt;- sapply(d$content, remove_reserved_characters, USE.NAMES = FALSE) write.xlsx(d, \u0026#34;test.xlsx\u0026#34;) 同時，若是運算環境有餘裕可以開啟平行運算，在 R 中我們使用 parallel package 來達到這個效果，可以將上面的程式碼替換成下面的模塊，透過平行運算可以大幅度得縮短運算時間。\nlibrary(parallel) library(doParallel) d \u0026lt;- read.csv(\u0026#34;/Users/hans/Downloads/textcsvdata.csv\u0026#34;) cl \u0026lt;- makeCluster(detectCores() - 1) registerDoParallel(cl) test \u0026lt;- foreach(x = d$content, .combine = c, .packages = c(\u0026#39;parallel\u0026#39;)) %dopar% remove_reserved_characters(x) stopCluster(cl) write.xlsx(d, \u0026#34;test.xlsx\u0026#34;) ","date":"12 July 2024","externalUrl":null,"permalink":"/posts/r_excelwrite/","section":"Posts","summary":"我們將在這篇文章中說明及解決在使用 R 寫入文本大數據至 Excel 時遇到的檔案損毀問題","title":"解決 Excel 寫入系統保留字元時導致的錯誤","type":"posts"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/series/flask/","section":"Series","summary":"","title":"Flask","type":"series"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/flask/","section":"Tags","summary":"","title":"Flask","type":"tags"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/categories/web-development/","section":"Categories","summary":"","title":"Web-Development","type":"categories"},{"content":"","date":"10 July 2024","externalUrl":null,"permalink":"/tags/web-framework/","section":"Tags","summary":"","title":"Web-Framework","type":"tags"},{"content":" 1. 介紹 # Flask 是什麼？ # Flask 是一個 Python 的網頁框架，用於構建輕量級的網頁應用程式。它由 Armin Ronacher 開發，基於 Werkzeug WSGI 工具和 Jinja2 模板引擎。並以簡單、靈活和可擴展性強著稱，因此適合各種規模的專案、簡單的個人網站到複雜的企業應用程式。\nFlask 的歷史和背景 # Flask 於 2010 年首次發布，旨在提供一個簡單易用的框架，讓開發者可以快速構建網頁應用程式。它被設計為「微框架」，這意味著它只提供最基本的功能，並允許開發者根據需要添加擴展和第三方模組。\nFlask 的適用範圍和特點 # Flask 的特點包括：\n輕量級和模組化：只包含基本功能，其他功能可以通過擴展來添加。 簡單的路由定義：使用裝飾器來定義 URL 路由。 強大的模板引擎：使用 Jinja2 模板引擎生成 HTML 頁面。 開發伺服器和偵錯工具：內建開發伺服器和偵錯工具，方便開發和測試。 豐富的擴展：有許多第三方擴展可用來增加功能，如資料庫 ORM、表單處理、驗證等。 範例應用場景 # 個人網站：利用 Flask 快速搭建一個簡單的個人網站。 企業應用：開發小型至中型企業應用，如內部管理系統或客戶管理系統。 API 服務：構建 RESTful API，提供數據服務。 2. 安裝與環境設置 # 安裝 Flask # 首先我們需要在電腦中安裝 Flask。在開始之前需要先安裝 Python。如果尚未安裝，可以從 Python 官方網站 下載並安裝。\n接著我們可以使用以下步驟來安裝 Flask：\n打開命令提示字元（Windows）或終端（macOS 和 Linux）。\n建立虛擬環境（可選，但強烈建議）在這裡我們以 conda 容器為例。\n# 建立虛擬環境 conda create --name myflask # 啟動虛擬環境 conda activate myflask 使用 pip 安裝 Flask：\npip install Flask 檢查安裝是否成功 # 我們可以使用以下命令來檢查 Flask 是否安裝成功：\n```bash python -m flask --version ``` 這會顯示 Flask 的版本號，如果一切正常，你應該會看到類似如下的輸出：\n``` Flask 2.x.x Python 3.x.x ``` 建立第一個 Flask 網頁 # 現在讓我們來建立一個簡單的 Flask 網頁，驗證我們的安裝是否成功。\n在你的項目目錄中創建一個 Python 文件，例如 app.py。\n在 app.py 中添加以下程式碼：\nfrom flask import Flask app = Flask(__name__) @app.route(\u0026#39;/\u0026#39;) def home(): return \u0026#34;Hello, Flask!\u0026#34; if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=True) 這段程式碼做了以下幾件事：\n從 Flask 導入 Flask 類別。 創建一個 Flask 應用實例 app。 使用 @app.route('/') 裝飾器定義了一個路由，當用戶訪問根 URL (/) 時，會調用 home 函數並返回 \u0026ldquo;Hello, Flask!\u0026quot;。 使用 app.run(debug=True) 啟動網頁的開發伺服器，並開啟偵錯模式。 啟動開發伺服器 # 透過在命令提示字元或終端中運行以下命令啟動開發伺服器：\n```bash python app.py ``` 我們會得到如下方的輸出：\n``` * Serving Flask app \u0026quot;app\u0026quot; (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: on * Running on http://127.0.0.1:8080/ (Press CTRL+C to quit) ``` 透過打開瀏覽器訪問 http://127.0.0.1:8080/ 後，頁面上會顯示我們剛剛在程式碼中輸入的 \u0026ldquo;Hello, Flask!\u0026quot;。\n小結 # 到此為止，我們已經安裝了 Flask，並成功創建並運行了一個簡單的 Flask 網頁。在接下來的章節中，我們將學習如何使用 Flask 的模板引擎來生成動態 HTML 頁面。\n在下一章節中，我們將介紹如何使用 Jinja2 模板引擎來建立和渲染動態的 HTML 頁面，讓我們的 Flask 網頁更具互動性和吸引力。\n","date":"10 July 2024","externalUrl":null,"permalink":"/posts/python_flask1/","section":"Posts","summary":"這篇文章教你如何安裝 Flask，並建立由簡單程式碼所架設的網頁","title":"快速上手 Flask：從安裝到第一個網頁","type":"posts"},{"content":"","date":"19 June 2024","externalUrl":null,"permalink":"/series/regex/","section":"Series","summary":"","title":"Regex","type":"series"},{"content":"","date":"19 June 2024","externalUrl":null,"permalink":"/tags/regex/","section":"Tags","summary":"","title":"Regex","type":"tags"},{"content":"","date":"19 June 2024","externalUrl":null,"permalink":"/tags/text-mining/","section":"Tags","summary":"","title":"Text-Mining","type":"tags"},{"content":"正則表達式（Regular Expressions，簡稱 regex）是一種強大的文本處理工具，允許開發者以高度靈活和效率的方式搜索、替換、分析和操作文本數據。在資料科學領域，正則表達式提供了一種解析大規模數據集、清洗數據和提取有價值信息的有效方法。\n正則表達式是用於描述字符串匹配模式的語言。它們主要用於字符串搜尋和替換操作，支持一些特定的語法規則，讓用戶能夠定義一個或多個匹配的規則。在資料科學中，正則表達式尤其重要，它們可以快速地從大量的文本數據中提取有用的信息，進而被廣泛地運用在許多程式語言中，並在不同的地方扮演著不同的角色，例如從日誌文件中提取日期和時間信息，或從社交媒體數據中識別和提取特定的話題標籤，甚至我們日常所使用的密碼設置時的大小寫識別等。若是沒有程式編譯環境，也可以透過regex101 這個網站來進行正則表達式的練習。\n本篇文章會簡單說明關於正則表達式的語法及應用，並且再以 R 做簡單的程式上使用的舉例。\n基本搜索 # 首先對於 R 的字串處理我們需要載入 stringr 這個 package，它是一個對於在 R 環境中進行文字探勘十分重要的套件。\ninstall.packages(\u0026#34;stringr\u0026#34;) library(stringr) ? # 在這個符號前的字符代表可有可無的意思，以下面為例，若我們想在句子中抓取包含 use 的詞，我們可以透過在 d 後面加入 ? 來使 use 的變體 used。\nsentence \u0026lt;- \u0026#34;use a used variable name is illegal.\u0026#34; str_extract_all(sentence, \u0026#34;used?\u0026#34;) # [[1]] # [1] \u0026#34;use\u0026#34; \u0026#34;used\u0026#34; * # 若是我們在搜索條件中加入這個條件，則代表在搜尋字串時需同時滿足三個條件才會被匡列成搜索對象：\n在搜索時除了不能出現其他字符外， * 前的字符只需「不需出現」、「出現一次」或「出現多次」其中之一即可。\n滿足前後字符的條件。\n以下面的例子來說，我們可以看到不論 b 出現多少次，都能正確的被正則表達式切割並抓取滿足條件之部分。但若是中間是 d，則因爲我們規定 a 和 c 中間只能出現零到多個 b，並不能出現其他字符，因此該字串不會被搜索。\nsentence \u0026lt;- c(\u0026#34;acd, abc, abbbbbcddddc, adc, addddddc\u0026#34;) str_extract_all(sentence, \u0026#34;ab*c\u0026#34;) # [[1]] # [1] \u0026#34;ac\u0026#34; \u0026#34;abc\u0026#34; \u0026#34;abbbbbc\u0026#34; + # 與上面不同的是，若我們希望 * 前的字符最少需出現一次，則我們可以使用 + 來做搜尋。下面是一個簡單的例子，我們可以看到在下面相較於上面的搜尋結果而言，\u0026lsquo;acd\u0026rsquo; 因為中間沒有 b 而沒有被抓取出。\nsentence \u0026lt;- c(\u0026#34;acd, abc, abbbbbcddddc, adc, addddddc\u0026#34;) str_extract_all(sentence, \u0026#34;ab+c\u0026#34;) # [[1]] # [1] \u0026#34;abc\u0026#34; \u0026#34;abbbbbc\u0026#34; () # 若是我們想在正則表達式中組合條件，我們可以使用 () 來達成需求，首先我們先來看到關於合併檢索的部分，下面是一個將 ab 組合後搜尋的例子，我們可以得到滿足大於等於出現一次 ab 條件的字符串。\nsentence \u0026lt;- c(\u0026#34;acd, ababc, abbbc, abababbbcddddc, adc, adadddddc\u0026#34;) str_extract_all(sentence, \u0026#34;(ab)+\u0026#34;) # [[1]] # [1] \u0026#34;abab\u0026#34; \u0026#34;ab\u0026#34; \u0026#34;ababab\u0026#34; 接著我們也可以透過加入 | 來執行搜索中「或」的操作，例如當我們想要得到大於等於出現一次 ab 或 ad 條件的字符串時可以進行下面的操作。\nsentence \u0026lt;- c(\u0026#34;acd, ababc, abbbc, abababbbcddddc, adc, adadddddc\u0026#34;) str_extract_all(sentence, \u0026#34;(ab|ad)+\u0026#34;) # [[1]] # [1] \u0026#34;abab\u0026#34; \u0026#34;ab\u0026#34; \u0026#34;ababab\u0026#34; \u0026#34;ad\u0026#34; \u0026#34;adad\u0026#34; {} # 在這裡若是我們想定義字符中出現的精確次數，我們可以透過加入 {} 及數字來做搜索。下面是一個簡單的範例。\nsentence \u0026lt;- c(\u0026#34;acd, abbc, abbbc, abbbbbcddddc, adc, addddddc\u0026#34;) str_extract_all(sentence, \u0026#34;ab{2}c\u0026#34;) # [[1]] # [1] \u0026#34;abbc\u0026#34; 而若是我們希望有一個搜索的範圍，我們可以在 {} 中加入區間來做範圍搜索。若我們想從段落中擷取出現 0~3 次 b 的字串，則簡單的範例如下。\nsentence \u0026lt;- c(\u0026#34;acd, abbc, abbbc, abbbbbcddddc, adc, addddddc\u0026#34;) str_extract_all(sentence, \u0026#34;ab{0,3}c\u0026#34;) # [[1]] # [1] \u0026#34;ac\u0026#34; \u0026#34;abbc\u0026#34; \u0026#34;abbbc\u0026#34; 同時我們希望指定一個搜索的方向，我們也可以在 {} 中只放入範圍的上界或下界來做搜索。若我們想從段落中擷取出現大於等於 3 次 b 的字串，我們可以進行如下的操作。\nsentence \u0026lt;- c(\u0026#34;acd, abbc, abbbc, abbbbbcddddc, adc, addddddc\u0026#34;) str_extract_all(sentence, \u0026#34;ab{3,}c\u0026#34;) # [[1]] # [1] \u0026#34;abbbc\u0026#34; \u0026#34;abbbbbc\u0026#34; [] # 若是我們想要搜索滿足特定字符條件的字串，我們可以使用 [] 來篩選字符，其中 a-z 與 A-Z 分別為代表大小寫字母的字符串，而 0-9 是代表數字的字符串。下面是我們的範例演示。\nsentence \u0026lt;- \u0026#34;abc, tiger, aabbcc, dog, 1234678, abc123456, ABCDEFG\u0026#34; str_extract_all(sentence, \u0026#34;[a-z]+\u0026#34;) # [[1]] # [1] \u0026#34;abc\u0026#34; \u0026#34;tiger\u0026#34; \u0026#34;aabbcc\u0026#34; \u0026#34;dog\u0026#34; \u0026#34;abc\u0026#34; sentence \u0026lt;- \u0026#34;abc, tiger, aabbcc, dog, 1234678, abc123456, ABCDEFG\u0026#34; str_extract_all(sentence, \u0026#34;[A-Z]+\u0026#34;) # [[1]] # [1] \u0026#34;ABCDEFG\u0026#34; sentence \u0026lt;- \u0026#34;abc, tiger, aabbcc, dog, 1234678, abc123456, ABCDEFG\u0026#34; str_extract_all(sentence, \u0026#34;[0-9]+\u0026#34;) # [[1]] # [1] \u0026#34;1234678\u0026#34; \u0026#34;123456\u0026#34; 同時若是要將條件混用只需將條件一同輸入即可。\nsentence \u0026lt;- \u0026#34;abc, tiger, aabbcc, dog, 1234678, abc123456, ABCDEFG\u0026#34; str_extract_all(sentence, \u0026#34;[A-Z0-9]+\u0026#34;) # [[1]] # [1] \u0026#34;1234678\u0026#34; \u0026#34;123456\u0026#34; \u0026#34;ABCDEFG\u0026#34; ^ # 若我們要添加除外的搜索條件，只需在條件前加上 ^ 即可，下面是在句子中剔除包含數字逗號及空格的表示法。\nsentence \u0026lt;- \u0026#34;abc, tiger, aabbcc, dog, 1234678, abc123456, ABCDEFG\u0026#34; str_extract_all(sentence, \u0026#34;[^0-9^,^ ]+\u0026#34;) # [[1]] # [1] \u0026#34;abc\u0026#34; \u0026#34;tiger\u0026#34; \u0026#34;aabbcc\u0026#34; \u0026#34;dog\u0026#34; \u0026#34;abc\u0026#34; \u0026#34;ABCDEFG\u0026#34; 元字符 # 在正則表達式中，我們一樣可以透過加入元字符來做條件的搜索，下面是一些常見的元字符及其定義，它們一樣可以透過加入上面的基本搜索來豐富我們的字串查詢系統。\n字符 定義 \\d 代表數字字符，意思等同於 [0-9] \\w 代表單詞字符，泛指英文、數字、下滑線 \\s 代表空白及換行字符 \\D 代表非數字字符，意思等同於 ^[0-9] \\W 代表非單詞字符 \\s 代表非空白及換行字符 . 代表任意字符，但不包含換行字符 透過這些簡單的正則表達式語法，我們可以很好的運用它們在 R 語言中進行文字資訊的抓取，接下來我們會在下一篇文章中提到關於正則表達式的進階應用。\n","date":"19 June 2024","externalUrl":null,"permalink":"/posts/code_regex/","section":"Posts","summary":"關於正則表達式在資料處理中的用法及示例","title":"使用正則表達式在 R 語言中挖掘文本資訊","type":"posts"},{"content":"","date":"14 June 2024","externalUrl":null,"permalink":"/tags/database/","section":"Tags","summary":"","title":"Database","type":"tags"},{"content":"","date":"14 June 2024","externalUrl":null,"permalink":"/tags/mongodb/","section":"Tags","summary":"","title":"Mongodb","type":"tags"},{"content":"","date":"14 June 2024","externalUrl":null,"permalink":"/series/mongodb/","section":"Series","summary":"","title":"MongoDB","type":"series"},{"content":"這是 MongoDB 的第三篇，我在本篇文章中將介紹如何利用 Python 連線至 MongoDB，並且一樣透過實際的程式演示來進行操作。\n首先我們需要先透過 pip 安裝 pymongo 我們透過它進行 Python 與 MongoDB 之間的連線，接著我們需要透過安裝 scikit-learn 來取得 iris 數據，scikit-learn 是一個關於在 Python 中運行機器學習的框架，其中也包含一些用於示範的資料，而 iris 數據便是其中之一，不過在這篇文章中我們只會將其用來做示範資料的讀取。最後我們需要安裝 pandas 來做資訊處理便於演示，pandas 是個在 Python 中負責資料處理的模塊，它擴充了原本不足的資料分析領域的功能。\npip install pymongo pip install scikit-learn pip install pandas 安裝完 pymongo 及 scikit-learn 後，我們可以先載入在 Python 中載入 iris 數據並將其轉換成 DataFrame 的格式便於後續的操作。\nfrom pymongo import MongoClient from sklearn.datasets import load_iris iris = load_iris() iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names) iris_df[\u0026#39;species\u0026#39;] = pd.Categorical.from_codes(iris.target, iris.target_names) print(iris_df.head()) 創建並寫入數據 # 首先我們在本地端的伺服器中創建了一個名為 mydatabase 的儲存空間，並創建了一個位於容器內，名稱為 iris 的資料表，並將這個目標位址資訊儲存在 target 這個變數中。\nclient = MongoClient(\u0026#34;mongodb://localhost:27017/\u0026#34;) db = client[\u0026#34;mydatabase\u0026#34;] target = db[\u0026#34;iris\u0026#34;] 接著我們使用 pandas 中的 to_dict(\u0026quot;records\u0026quot;) 將原本的 DataFrame 轉換成 Dict 的 Key-Value 形式，便於將資料寫入 NoSQL 中，然後我們便可以用 pymongo 中的 insert_many 函數將 iris 資料表的所有內容寫入目標位址了。\niris_dict = iris_df.to_dict(\u0026#34;records\u0026#34;) target.insert_many(iris_dict) 從資料庫中讀取 # 由於我們上面已經將目標位址儲存在 target 這個變數中，因此當我們需要調用他時，只需要使用 pymongo 中的 find 函數就可以找出目標位址的資料，同時我們可以使用 limit 來限制查找的筆數，確認資料有順利寫入至資料庫中。\nprint(\u0026#34;Inserted data:\u0026#34;) for doc in target.find().limit(6): print(doc) # Inserted data: # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab306210638\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 5.1, \u0026#39;sepal width (cm)\u0026#39;: 3.5, \u0026#39;petal length (cm)\u0026#39;: 1.4, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;setosa\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab306210639\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 4.9, \u0026#39;sepal width (cm)\u0026#39;: 3.0, \u0026#39;petal length (cm)\u0026#39;: 1.4, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;setosa\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621063a\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 4.7, \u0026#39;sepal width (cm)\u0026#39;: 3.2, \u0026#39;petal length (cm)\u0026#39;: 1.3, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;setosa\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621063b\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 4.6, \u0026#39;sepal width (cm)\u0026#39;: 3.1, \u0026#39;petal length (cm)\u0026#39;: 1.5, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;setosa\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621063c\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 5.0, \u0026#39;sepal width (cm)\u0026#39;: 3.6, \u0026#39;petal length (cm)\u0026#39;: 1.4, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;setosa\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621063d\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 5.4, \u0026#39;sepal width (cm)\u0026#39;: 3.9, \u0026#39;petal length (cm)\u0026#39;: 1.7, \u0026#39;petal width (cm)\u0026#39;: 0.4, \u0026#39;species\u0026#39;: \u0026#39;setosa\u0026#39;} 對資料庫寫入新數據 # 若我們需要將一筆新數據繼續寫入同一個資料表，那麼我們只需要鎖定位址後，繼續使用 insert_one 函數寫入即可，例如我們可以再執行一次下面的寫入操作，\nnew_data = { \u0026#34;sepal length (cm)\u0026#34;: 5.0, \u0026#34;sepal width (cm)\u0026#34;: 3.6, \u0026#34;petal length (cm)\u0026#34;: 1.4, \u0026#34;petal width (cm)\u0026#34;: 0.2, \u0026#34;species\u0026#34;: \u0026#34;setosa\u0026#34; } target.insert_one(new_data) # InsertOneResult(ObjectId(\u0026#39;666be4ee79402ab3062106ce\u0026#39;), acknowledged=True) 完成寫入後，我們將得到一個在加入新資料後資料總筆數為151筆的資料表，同時我們一樣可以可以用函數確認資料的筆數確實有151筆。\ncount = target.count_documents({}) print(f\u0026#34;Total number of documents: {count}\u0026#34;) # Total number of documents: 151 更新資料表內的數據 # 但我們有時可能會想更新資料表中的部分數據，若是我們想根據特定條件來對局部符合條件的數據進行替換、覆蓋或刪除時，我們可以採用下面的做法：\n替換單筆數據 # 不同於 R，在 Python 中我們可以分別使用 replace_one 函數來替換資料表中「符合搜尋條件」的「第一筆」數據，下面是一個簡單的例子，讓我來簡單的說明他：\n我將要用於替換的數據使用 Dict 的 Key-Value 格式存入 new_data 這個變數中。\nquery 這個變數是用於搜尋的索引條件，在這裡我們一樣需要使用 Dict 的 Key-Value 格式來儲存。\n最後我們將上面新增的變數使用 replace_one 函數來替換資料表中的數據。\nnew_data = { \u0026#34;sepal length (cm)\u0026#34;: 5.1, \u0026#34;sepal width (cm)\u0026#34;: 3.5, \u0026#34;petal length (cm)\u0026#34;: 1.4, \u0026#34;petal width (cm)\u0026#34;: 0.2, \u0026#34;species\u0026#34;: \u0026#34;versicolor\u0026#34; } query = {\u0026#34;species\u0026#34;: \u0026#34;setosa\u0026#34;} result = target.replace_one(query, new_data) 在完成替換後，我們可以透過下面的程式查詢替換後的結果，並可以發現第一筆數據的鳶尾花種類變成了 versicolor。\nprint(\u0026#34;Data after replacement:\u0026#34;) for doc in target.find().limit(6): print(doc) # Data after replacement: # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab306210638\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 5.1, \u0026#39;sepal width (cm)\u0026#39;: 3.5, \u0026#39;petal length (cm)\u0026#39;: 1.4, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;versicolor\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab306210639\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 4.9, \u0026#39;sepal width (cm)\u0026#39;: 3.0, \u0026#39;petal length (cm)\u0026#39;: 1.4, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;setosa\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621063a\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 4.7, \u0026#39;sepal width (cm)\u0026#39;: 3.2, \u0026#39;petal length (cm)\u0026#39;: 1.3, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;setosa\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621063b\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 4.6, \u0026#39;sepal width (cm)\u0026#39;: 3.1, \u0026#39;petal length (cm)\u0026#39;: 1.5, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;setosa\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621063c\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 5.0, \u0026#39;sepal width (cm)\u0026#39;: 3.6, \u0026#39;petal length (cm)\u0026#39;: 1.4, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;setosa\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621063d\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 5.4, \u0026#39;sepal width (cm)\u0026#39;: 3.9, \u0026#39;petal length (cm)\u0026#39;: 1.7, \u0026#39;petal width (cm)\u0026#39;: 0.4, \u0026#39;species\u0026#39;: \u0026#39;setosa\u0026#39;} 覆蓋數據 # 面對一筆以上需要替換的數據，若我們使用 replace_one 來一一替換顯得十分沒有效率，這時我們可以透過 update＿many 這個函數來實現批量操作的效果，下面是我的程式說明及範例程式碼：\n我將要用於替換的數據使用 Dict 格式的字串存入 update_data 這個變數中。\n在這裏同樣以 query 這個變數是用於搜尋的索引條件，同時也需要使用 Dict 格式的字串來儲存。\n最後我們將上面新增的變數使用 update_many 函數來同時替換資料表中符合條件的所有資料。\nquery = {\u0026#34;species\u0026#34;: \u0026#34;setosa\u0026#34;} update_data = {\u0026#34;$set\u0026#34;: {\u0026#34;species\u0026#34;: \u0026#34;virginica\u0026#34;}} result = target.update_many(query, update_data) 在完成更新後我們也可以透過下面的程式查詢替換後的結果，並可以發現所有種類為setosa的鳶尾花，它們的種類均變成了 virginica。\nprint(\u0026#34;Data after update:\u0026#34;) for doc in target.find().limit(6): print(doc) # Data after update: # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab306210638\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 5.1, \u0026#39;sepal width (cm)\u0026#39;: 3.5, \u0026#39;petal length (cm)\u0026#39;: 1.4, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;versicolor\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab306210639\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 4.9, \u0026#39;sepal width (cm)\u0026#39;: 3.0, \u0026#39;petal length (cm)\u0026#39;: 1.4, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;virginica\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621063a\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 4.7, \u0026#39;sepal width (cm)\u0026#39;: 3.2, \u0026#39;petal length (cm)\u0026#39;: 1.3, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;virginica\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621063b\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 4.6, \u0026#39;sepal width (cm)\u0026#39;: 3.1, \u0026#39;petal length (cm)\u0026#39;: 1.5, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;virginica\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621063c\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 5.0, \u0026#39;sepal width (cm)\u0026#39;: 3.6, \u0026#39;petal length (cm)\u0026#39;: 1.4, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;virginica\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621063d\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 5.4, \u0026#39;sepal width (cm)\u0026#39;: 3.9, \u0026#39;petal length (cm)\u0026#39;: 1.7, \u0026#39;petal width (cm)\u0026#39;: 0.4, \u0026#39;species\u0026#39;: \u0026#39;virginica\u0026#39;} 刪除數據 # 當我們想從資料表中移除所有滿足特定條件的資料時，我們可以使用 delete_many 函數來對資料庫進行操作，接著我們開始範例程式碼的說明：\n我們以 query 這個 Dict 格式的變數來設定要刪除資料的索引條件，同時也需要使用字串形式來儲存它。\n接著我們將上面鎖定的變數使用 delete_many 函數來移除資料表中的所有滿足條件之數據。\nquery = {\u0026#34;species\u0026#34;: \u0026#34;virginica\u0026#34;} result = target.delete_many(query) 透過執行下面的程式，我們可以發現所有種類為 virginica 的鳶尾花均被從資料表中移除了。\nprint(\u0026#34;Remaining data:\u0026#34;) for doc in target.find().limit(6): print(doc) print(f\u0026#34;Documents deleted: {result.deleted_count}\u0026#34;) # Remaining data: # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab306210638\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 5.1, \u0026#39;sepal width (cm)\u0026#39;: 3.5, \u0026#39;petal length (cm)\u0026#39;: 1.4, \u0026#39;petal width (cm)\u0026#39;: 0.2, \u0026#39;species\u0026#39;: \u0026#39;versicolor\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621066a\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 7.0, \u0026#39;sepal width (cm)\u0026#39;: 3.2, \u0026#39;petal length (cm)\u0026#39;: 4.7, \u0026#39;petal width (cm)\u0026#39;: 1.4, \u0026#39;species\u0026#39;: \u0026#39;versicolor\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621066b\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 6.4, \u0026#39;sepal width (cm)\u0026#39;: 3.2, \u0026#39;petal length (cm)\u0026#39;: 4.5, \u0026#39;petal width (cm)\u0026#39;: 1.5, \u0026#39;species\u0026#39;: \u0026#39;versicolor\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621066c\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 6.9, \u0026#39;sepal width (cm)\u0026#39;: 3.1, \u0026#39;petal length (cm)\u0026#39;: 4.9, \u0026#39;petal width (cm)\u0026#39;: 1.5, \u0026#39;species\u0026#39;: \u0026#39;versicolor\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621066d\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 5.5, \u0026#39;sepal width (cm)\u0026#39;: 2.3, \u0026#39;petal length (cm)\u0026#39;: 4.0, \u0026#39;petal width (cm)\u0026#39;: 1.3, \u0026#39;species\u0026#39;: \u0026#39;versicolor\u0026#39;} # {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;666bc5c379402ab30621066e\u0026#39;), \u0026#39;sepal length (cm)\u0026#39;: 6.5, \u0026#39;sepal width (cm)\u0026#39;: 2.8, \u0026#39;petal length (cm)\u0026#39;: 4.6, \u0026#39;petal width (cm)\u0026#39;: 1.5, \u0026#39;species\u0026#39;: \u0026#39;versicolor\u0026#39;} 移除整個資料表 # 在移除資料表前，我們先查詢 mydatabase 資料庫中有哪些資料表，並印出他們的名稱。我們可以看到裡面包含我們欲刪除的資料表 iris。\ncollections_before = db.list_collection_names() print(\u0026#34;Collections before dropping:\u0026#34;, collections_before) # Collections before dropping: [\u0026#39;iris\u0026#39;] 最後，當我們想要從資料庫中移除整個資料表，由于我們前面已經定義了變數 target 為 iris 資料表的位址，因此我們可以透過 drop 函數來將整個資料表從目標位址中移除，下面是簡單的程式範例：\nif \u0026#34;iris\u0026#34; in collections_before: db[\u0026#34;iris\u0026#34;].drop() print(\u0026#34;Iris collection dropped.\u0026#34;) else: print(\u0026#34;Iris collection does not exist.\u0026#34;) # Iris collection dropped. 在移除完畢後，我們可以從下面的程式碼搜尋資料庫的指定位址中所有資料表的名稱，因為我們剛剛將位於 mydatabase 中的 iris 移除了，因此資料庫中已沒有其他資料表，故執行程式後會得到 [] 的結果。\ncollections_after = db.list_collection_names() print(\u0026#34;Collections after dropping:\u0026#34;, collections_after) # Collections after dropping: [] 到這裡我們便完成了在 Python 中對 MongoDB 基本的 CRUD 操作。\n","date":"14 June 2024","externalUrl":null,"permalink":"/posts/sql_mongodb_py/","section":"Posts","summary":"以 Python 連線至本地端的 MongoDB，並且結合 pandas 來演示基本的 CRUD 操作","title":"如何使用 Python 處理 MongoDB 中的數據","type":"posts"},{"content":"接續我們前面所建構的 MongoDB Server，我在本篇文章中將介紹如何利用 R 連線至 MongoDB，並且透過實際的程式演示來進行 MongoDB 的連線與執行基本的 CRUD（創建、讀取、更新、刪除）操作。\n首先我們需要安裝並載入 mongolite package，我們透過它進行 R 與 MongoDB 之間的連線。由於我們在上一篇文章中只有一個空的資料庫，為了能更好的演示，接下來我將以 R 內建的 iris 數據作為操作的範例。\ninstall.packages(\u0026#34;mongolite\u0026#34;) library(mongolite) 安裝完 mongolite 後，我們可以先載入 iris 數據便於後續的操作。\ndata(iris) 創建並寫入數據 # 首先我們在本地端的伺服器中創建了一個名為 mydatabase 的儲存空間，並創建了一個位於容器內，名稱為 iris 的資料表，並將這個目標位址資訊儲存在 target 這個變數中。接著我們利用 mongolite 中的 insert 函數將內建的 iris 資料表寫入目標位址中。\ntarget \u0026lt;- mongo(collection = \u0026#34;iris\u0026#34;, db = \u0026#34;mydatabase\u0026#34;, url = \u0026#34;mongodb://localhost\u0026#34;) target$insert(iris) 從資料庫中讀取 # 由於我們上面已經將目標位址儲存在 target 這個變數中，因此當我們需要調用他時，只需要使用 find 函數就可以找出目標位址的資料，同時我們可以使用 head 來查看前幾筆的資料，並確認資料有順利寫入至資料庫中，同時我們可以使用 dim 函數知道寫入的資料有150筆，變數有5個。\niris_data \u0026lt;- target$find() head(iris_data) # Sepal_Length Sepal_Width Petal_Length Petal_Width Species # 1 5.1 3.5 1.4 0.2 setosa # 2 4.9 3.0 1.4 0.2 setosa # 3 4.7 3.2 1.3 0.2 setosa # 4 4.6 3.1 1.5 0.2 setosa # 5 5.0 3.6 1.4 0.2 setosa # 6 5.4 3.9 1.7 0.4 setosa dim(iris_data) # [1] 150 5 對資料庫寫入新數據 # 若我們只是需要將新數據繼續寫入同一個資料表，那麼我們只需要鎖定位址後，繼續使用 insert 函數寫入即可，例如我們可以再執行一次下面的寫入操作，\nnew_data \u0026lt;- data.frame(Sepal.Length = 5.0, Sepal.Width = 3.6, Petal.Length = 1.4, Petal.Width = 0.2, Species = \u0026#34;setosa\u0026#34;) target$insert(new_data) 完成寫入後，我們將得到一個在加入新資料後資料總筆數為151筆的資料表，同時我們可以用 nrow 函數確認資料的筆數確實有151筆。\nnrow(iris_data) # [1] 151 更新資料表內的數據 # 但我們有時可能會想更新資料表中的部分數據，若是我們想根據特定條件來對局部符合條件的數據進行替換、覆蓋或刪除時，我們可以採用下面的做法：\n替換數據 # 我們可以使用 replace 函數來替換資料表中「符合搜尋條件」的「第一筆」數據，下面是一個簡單的例子，讓我來簡單的說明他：\n我將要用於替換的數據使用 JSON 格式的字串存入 new_data 這個變數中。\nquery 這個變數是用於搜尋的索引條件，在這裡我們一樣需要使用 JSON 格式的字串來儲存。\n最後我們將上面新增的變數使用 replace 函數來替換資料表中的數據。\nnew_data \u0026lt;- \u0026#39;{\u0026#34;Sepal_Length\u0026#34;: 5.1, \u0026#34;Sepal_Width\u0026#34;: 3.5, \u0026#34;Petal_Length\u0026#34;: 1.4, \u0026#34;Petal_Width\u0026#34;: 0.2, \u0026#34;Species\u0026#34;: \u0026#34;versicolor\u0026#34;}\u0026#39; query \u0026lt;- \u0026#39;{\u0026#34;Species\u0026#34;: \u0026#34;setosa\u0026#34;}\u0026#39; target$replace(query, new_data) 在完成替換後，我們可以透過下面的程式查詢替換後的結果，並可以發現第一筆數據的鳶尾花種類變成了 versicolor。\niris_data \u0026lt;- target$find() head(iris_data) # Sepal_Length Sepal_Width Petal_Length Petal_Width Species # 1 5.1 3.5 1.4 0.2 versicolor # 2 4.9 3.0 1.4 0.2 setosa # 3 4.7 3.2 1.3 0.2 setosa # 4 4.6 3.1 1.5 0.2 setosa # 5 5.0 3.6 1.4 0.2 setosa # 6 5.4 3.9 1.7 0.4 setosa 覆蓋數據 # 面對一筆以上需要替換的數據，若我們使用 replace 來一一替換顯得十分沒有效率，這時我們可以透過 update 這個函數來實現批量操作的效果，下面是我的程式說明及範例程式碼：\n我將要用於替換的數據使用 JSON 格式的字串存入 renew 這個變數中，在這裡我加入了 \u0026ldquo;$set\u0026rdquo; 來幫助我索引替換的變數位址而不是將整個資料表的所有變數完全替換成單一值。\n在這裏同樣以 query 這個變數是用於搜尋的索引條件，同時也需要使用 JSON 格式的字串來儲存。\n最後我們將上面新增的變數使用 update 函數來替換資料表中的數據，與前面不同的是，我加入了 multiple = TRUE 這個參數來讓更新的操作對資料表中符合條件的所有資料都可以同步進行。\nrenew \u0026lt;- \u0026#39;{\u0026#34;$set\u0026#34;: {\u0026#34;Species\u0026#34;: \u0026#34;virginica\u0026#34;}}\u0026#39; query \u0026lt;- \u0026#39;{\u0026#34;Species\u0026#34;: \u0026#34;setosa\u0026#34;}\u0026#39; target$update(query, renew, multiple = TRUE) 在完成更新後我們也可以透過下面的程式查詢替換後的結果，並可以發現所有種類為setosa的鳶尾花，它們的種類均變成了 virginica。\niris_data \u0026lt;- target$find() head(iris_data) # Sepal_Length Sepal_Width Petal_Length Petal_Width Species # 1 5.1 3.5 1.4 0.2 versicolor # 2 4.9 3.0 1.4 0.2 virginica # 3 4.7 3.2 1.3 0.2 virginica # 4 4.6 3.1 1.5 0.2 virginica # 5 5.0 3.6 1.4 0.2 virginica # 6 5.4 3.9 1.7 0.4 virginica 刪除數據 # 當我們想從資料表中移除所有滿足特定條件的資料時，我們可以使用 remove 函數來對資料庫進行操作，接著我們開始範例程式碼的說明：\n我們以 query 這個 JSON 格式的變數來設定要刪除資料的索引條件，同時也需要使用字串形式來儲存它。\n接著我們將上面鎖定的變數使用 remove 函數來移除資料表中的所有滿足條件之數據。\nquery \u0026lt;- \u0026#39;{\u0026#34;Species\u0026#34;: \u0026#34;virginica\u0026#34;}\u0026#39; target$remove(query) 透過執行下面的程式，我們可以發現所有種類為 virginica 的鳶尾花均被從資料表中移除了。\niris_data \u0026lt;- target$find() head(iris_data) # Sepal_Length Sepal_Width Petal_Length Petal_Width Species # 1 5.1 3.5 1.4 0.2 versicolor # 2 7.0 3.2 4.7 1.4 versicolor # 3 6.4 3.2 4.5 1.5 versicolor # 4 6.9 3.1 4.9 1.5 versicolor # 5 5.5 2.3 4.0 1.3 versicolor # 6 6.5 2.8 4.6 1.5 versicolor 移除整個資料表 # 在移除資料表前，我們先查詢 mydatabase 資料庫中有哪些資料表，並印出他們的名稱。我們可以看到裡面包含我們欲刪除的資料表 iris。\ndb_connection \u0026lt;- mongo(url = \u0026#34;mongodb://localhost/mydatabase\u0026#34;) collections \u0026lt;- db_connection$run(\u0026#39;{\u0026#34;listCollections\u0026#34;:1}\u0026#39;) collection_names \u0026lt;- collections$cursor$firstBatch$name print(collection_names) # [1] \u0026#34;iris\u0026#34; 最後，當我們想要從資料庫中移除整個資料表，由于我們前面已經定義了變數 target 為 iris 資料表的位址，因此我們可以透過 drop 函數來將整個資料表從目標位址中移除，下面是簡單的程式範例：\ntarget$drop() 在移除完畢後，我們可以從下面的程式碼搜尋資料庫的指定位址中所有資料表的名稱，因為我們剛剛將位於 mydatabase 中的 iris 移除了，因此資料庫中已沒有其他資料表，故執行程式後會得到 NULL 的結果表示空值的意思。\ndb_connection \u0026lt;- mongo(url = \u0026#34;mongodb://localhost/mydatabase\u0026#34;) collections \u0026lt;- db_connection$run(\u0026#39;{\u0026#34;listCollections\u0026#34;:1}\u0026#39;) collection_names \u0026lt;- collections$cursor$firstBatch$name print(collection_names) # NULL 到這裡我們便完成了在 R 中對 MongoDB 基本的 CRUD 操作。\n","date":"13 June 2024","externalUrl":null,"permalink":"/posts/sql_mongodb_r/","section":"Posts","summary":"使用 R 連線至本地端的 MongoDB，並透過實際的程式演示來進行資料庫的基本 CRUD 操作","title":"如何使用 R 處理 MongoDB 中的數據","type":"posts"},{"content":"","date":"12 June 2024","externalUrl":null,"permalink":"/tags/nosql/","section":"Tags","summary":"","title":"Nosql","type":"tags"},{"content":"MongoDB 是一種 NoSQL 資料庫，以其高性能、靈活的資料模型和良好的可擴展性而廣受歡迎。與傳統的關聯式資料庫不同，MongoDB 使用類似 JSON 格式的文件來存儲資料，使得它特別適合處理多變且非結構化的數據。在現代應用程序中，尤其是需要處理大數據和實時分析的場景下，MongoDB 提供了優異的解決方案。\n因此我計劃撰寫一部 MongoDB 的入門指南，藉由實際操作來學習 MongoDB 為自己培養新的技能。本篇文章將介紹如何在 Mac 電腦上部署 MongoDB，透過本地端的部署可以讓我們後續橋接程式使用構建開發環境。\n安装Homebrew # Homebrew 是一個開源的安裝包管理器，專為 macOS 和 Linux 系統設計。它讓用戶能夠輕鬆地在終端幾上安裝、更新和管理各種插件和工具。透過 Homebrew 我們可以將工具安裝過程簡化，避免用戶手動處理各插件之間的依賴關係和繁瑣的配置文件。通過簡單的命令如 brew install，使用戶可以輕鬆獲取和安裝他們需要的工具，讓開發者和系統管理員能夠更高效地管理電腦內的環境。\n在開始安裝MongoDB前，我們可以透過以下命令安装並更新Homebrew至最新版本，便於我們後續從 Homebrew 上下載 MongoDB：\n/bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; brew update 透過 Homebrew 下載及使用 MongoDB # 接著我們將透過 Homebrew 下載 MongoDB community 的 5.0 版本。\nbrew tap mongodb/brew brew install mongodb-community@5.0 當安裝完成後，我們可以用下面的指令啟動 MongoDB。\nbrew services start mongodb/brew/mongodb-community 安裝完畢之後我們可以透過檢查 MongoDB 的版本來確認是否有順利完成安裝。\nmongo --version 如果順利安裝，我們可以終端機中看到相似於下列的訊息。\nMongoDB shell version v5.0.27 Build Info: { \u0026#34;version\u0026#34;: \u0026#34;5.0.27\u0026#34;, \u0026#34;gitVersion\u0026#34;: \u0026#34;49571988f1fea870e803f71a3ef8417173f3fbb1\u0026#34;, \u0026#34;modules\u0026#34;: [], \u0026#34;allocator\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;environment\u0026#34;: { \u0026#34;distarch\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;target_arch\u0026#34;: \u0026#34;x86_64\u0026#34; } } 若是需要「重新啟動」或是「關閉」運行中的 Mongodb 我們只需運行下面的指令便能做到。\nbrew services restart mongodb/brew/mongodb-community brew services stop mongodb/brew/mongodb-community 到這裡我們便完成了本地端的資料庫部署，我們會在這個系列的下一篇文章中示範如何使用程式語言連線至 Mongodb 進行操作。\n附錄：安裝中可能遇到的問題 # 如果在啟動 MongoDB後出現下面的訊息，表示 MongoDB 雖然已經順利安裝並運行，但它没有被自動連接到你的 PATH 環境中。所以我們需要手動將 MongoDB 的路徑新增至 PATH 環境變數中。\nmongodb-community@5.0 is keg-only, which means it was not symlinked into /opt/homebrew, because this is an alternate version of another formula. If you need to have mongodb-community@5.0 first in your PATH, run: echo \u0026#39;export PATH=\u0026#34;/opt/homebrew/opt/mongodb-community@5.0/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc To start mongodb/brew/mongodb-community@5.0 now and restart at login: brew services start mongodb/brew/mongodb-community@5.0 我們可以透過編輯 .zshrc 文件將 MongoDB 的路徑新增到 PATH 環境變數中，接著我們重新載入 .zshrc 文件後便可以成功將 MongoDB 的新路徑加入 PATH 環境變數中。\necho \u0026#39;export PATH=\u0026#34;/opt/homebrew/opt/mongodb-community@5.0/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc ","date":"12 June 2024","externalUrl":null,"permalink":"/posts/sql_mongodb/","section":"Posts","summary":"在這篇文章中我將介紹如何在 Mac 電腦上部署本地端的 MongoDB","title":"如何在 Mac 上部署 MongoDB Server","type":"posts"},{"content":"","date":"9 June 2024","externalUrl":null,"permalink":"/categories/data-science/","section":"Categories","summary":"","title":"Data-Science","type":"categories"},{"content":"","date":"9 June 2024","externalUrl":null,"permalink":"/tags/modeling/","section":"Tags","summary":"","title":"Modeling","type":"tags"},{"content":"","date":"9 June 2024","externalUrl":null,"permalink":"/tags/statistics/","section":"Tags","summary":"","title":"Statistics","type":"tags"},{"content":" Introduction # 這篇文章是修改我碩一時所製作的 R 函數，當時我們的目標是想要以Ｒ語言實作基於 F-value 的 Forward 及 Backward Stepwise Model Selection，由於近期在使用上套用至部分資料會跑出錯誤訊息，因此在這裏做簡單的版本翻修，並新增進了設置停止區間的參數及支援顯示當前進度的訊息，接下來我會簡單講解這個函數的參數及使用方法，更詳細的資訊可以參考下面的 Github 連結。\nhans0803/APLM R 1 0 fselect 變數列表 # y : 放置被預測的變量\ndata : 放置用於預測y的變數\nalpha_in : 設置 alpha 值用於判斷變數的 SLR 的 p-value 是否符合將變數放入模型的標準\nalpha_out : 設置 alpha 值用於判斷變數在模型中的 p-value 是否符合將變數變數從模型中移出的標準\nmode : 可以選擇forward或backward兩種\nearly_break_count : 設置當變數連續進出模型指定次數時將會中斷變數挑選，為了避免特定幾個變數輪流加入移出模型造成無法結束迴圈的情況發生\nfselect 函數說明 # 我們透過 F-value 的 Forward 及 Backward Stepwise Model Selection 來對模型進行變數挑選，不同於Ｒ內建的 aic 選模，我們嘗試提供另一種變數挑選的方法，同時在變數挑選完畢後，我們會使用 Vif test 對被選入的變數進行一次初步的共線性檢查，確保模型的變數是可以被較好的解釋的。\n面對僅有300個樣本，卻有6000個變數之類的高維度數據時，Ｒ內建的 aic 選模無法處理，但 fselect 支援對變數大於資料樣本時模型時的變數初步篩選，讓面對高維度數據時可以快速的縮小變數數目利於後續的特徵工程與建模。\n完整的 package 在 R 中透過以下指令下載：\nif(!require(devtools)) install.packages(\u0026#34;devtools\u0026#34;) devtools::install_github(\u0026#34;hans0803/APLM\u0026#34;) Code # 在文末附上完整的函數。\nfselect \u0026lt;- function(y, data, alpha_in=0.01, alpha_out=0.05, mode=\u0026#34;forward\u0026#34;, early_break_count=10){ if(mode==\u0026#34;forward\u0026#34;){ # start { # Set alpha, in \u0026lt; out # define in and out set to save who should in model now df_in \u0026lt;- data.frame() df_out \u0026lt;- data # Get the variable num and define no ones in df_in model out_width \u0026lt;- ncol(df_out) in_width \u0026lt;- 0 # loop run condition continue \u0026lt;- TRUE break_count \u0026lt;- 0 who_in \u0026lt;- \u0026#39;\u0026#39; who_out \u0026lt;- \u0026#39;\u0026#39; } while(continue){ # when we into the loop, set continue = FALSE continue \u0026lt;- FALSE # create note and check it { # Create the empty vector to save names, F-values, P-values C_names \u0026lt;- c(); F_value \u0026lt;- c(); P_value \u0026lt;- c() # Do i times SLR create the information data.frame \u0026#34;note\u0026#34; for(i in 1:out_width){ choose_data \u0026lt;- as.data.frame(df_out[,i]) if(in_width!=0){ test_f \u0026lt;- cbind(df_in, choose_data) }else{ test_f \u0026lt;- choose_data } fit \u0026lt;- lm(y ~ ., test_f) aov \u0026lt;- anova(fit) C_names[i] \u0026lt;- colnames(df_out[i]) F_value[i] \u0026lt;- aov$`F value`[in_width+1] P_value[i] \u0026lt;- aov$`Pr(\u0026gt;F)`[in_width+1] } note \u0026lt;- data.frame(name=C_names, F_value=F_value, P_value=P_value) # Find whos F-value is biggest and take it P-value max_F \u0026lt;- max(note$F_value) max_Fvar \u0026lt;- which(note$F_value==max_F)[1] alpha_test \u0026lt;- as.numeric(note$P_value[max_Fvar]) } print(alpha_test) # if it pass the check, add in and drop out { # Catch it in to model if(alpha_test \u0026lt;= alpha_in){ # if the biggest F-value variable can be catch, we set countinue = TRUE continue \u0026lt;- TRUE # get the biggest F-value variable who_catch \u0026lt;- note$name[max_Fvar] catch_var \u0026lt;- which(colnames(df_out)==who_catch) catch_data \u0026lt;- as.data.frame(df_out[, catch_var]) colnames(catch_data) \u0026lt;- who_catch # find it and add it from the df_out data.frame if(in_width!=0){ df_in \u0026lt;- cbind(catch_data, df_in) }else{ df_in \u0026lt;- as.data.frame(catch_data) colnames(df_in) \u0026lt;- who_catch } # find who into the df_in, and delete it from df_out if(out_width!=1){ df_out[, who_catch] \u0026lt;- c() }else{ df_out \u0026lt;- data.frame() } # since one variable move to df_in from df_out, change the width of df out_width \u0026lt;- out_width-1 in_width \u0026lt;- in_width +1 cat(\u0026#39;in:\u0026#39;, in_width, \u0026#39;/ out:\u0026#39;, out_width, \u0026#39;:\u0026#39;, who_catch, \u0026#39;(IN) \\n\u0026#39;) } } who_in \u0026lt;- who_catch if(who_in==who_out){ break_count \u0026lt;- break_count + 1 }else{ break_count \u0026lt;- 0 } if(break_count \u0026gt; early_break_count){ message_var \u0026lt;- paste(\u0026#34;Break warning by variable in=out\u0026#34;) warning(message_var) break } # stepwise part start # check df_in variable \u0026gt; 1 if(in_width \u0026gt; 1){ # delete this loop new variable if(in_width!=2){ step_df \u0026lt;- df_in[,-1] }else{ save_name \u0026lt;- colnames(df_in)[2] step_df \u0026lt;- as.data.frame(df_in[,-1]) colnames(step_df) \u0026lt;- save_name } # create information of df_in model to \u0026#34;forward_note\u0026#34; forward_fit \u0026lt;- lm(y ~ ., step_df) # use another ones to anova forward_aov \u0026lt;- anova(forward_fit) C_names \u0026lt;- colnames(step_df) F_value \u0026lt;- forward_aov$`F value`[1:in_width-1] # in_width-1, since we delete this loop new variable P_value \u0026lt;- forward_aov$`Pr(\u0026gt;F)`[1:in_width-1] # in_width-1, since we delete this loop new variable forward_note \u0026lt;- data.frame(name=C_names, F_value=F_value, P_value=P_value) # Find whos F-value is smallest and take it P-value min_F \u0026lt;- min(forward_note$F_value) min_Fvar \u0026lt;- which(forward_note$F_value==min_F)[1] print(forward_note$name[min_Fvar]) alpha_test \u0026lt;- as.numeric(forward_note$P_value[min_Fvar]) print(alpha_test) if(alpha_test \u0026gt; alpha_out){ # if the smallest F-value variable can be drop, we set countinue = TRUE continue \u0026lt;- TRUE # get the smallest F-value variable who_catch \u0026lt;- forward_note$name[min_Fvar] drop_var \u0026lt;- which(colnames(df_in)==who_catch) drop_data \u0026lt;- as.data.frame(df_in[, drop_var]) colnames(drop_data) \u0026lt;- who_catch # find it and add it to the df_out data.frame if(out_width!=0){ df_out \u0026lt;- cbind(drop_data, df_out) }else{ df_out \u0026lt;- drop_data } # find who into the df_out, and delete it from df_in df_in[, drop_var] \u0026lt;- c() # since one variable move to df_out from df_in, change the width of df out_width \u0026lt;- out_width+1 in_width \u0026lt;- in_width -1 cat(\u0026#39;in:\u0026#39;, in_width, \u0026#39;/ out:\u0026#39;, out_width, \u0026#39;:\u0026#39;, who_catch, \u0026#39;(OUT) \\n\u0026#39;) } } who_out \u0026lt;- who_catch # stepwise part end # if no any variable in df_out, stop loop if(out_width==0){ break } } } if(mode==\u0026#34;backward\u0026#34;){ # start { # Set alpha, in \u0026lt; out # define in and out set to save who should in model now df_in \u0026lt;- data df_out \u0026lt;- data.frame() save \u0026lt;- alpha_in alpha_in \u0026lt;- alpha_out alpha_out \u0026lt;- save # Get the variable num and define no ones in df_in model out_width \u0026lt;- 0 in_width \u0026lt;- ncol(df_in) # loop run condition continue \u0026lt;- TRUE break_count \u0026lt;- 0 who_in \u0026lt;- \u0026#39;\u0026#39; who_out \u0026lt;- \u0026#39;\u0026#39; } while(continue){ # when we into the loop, set continue = FALSE continue \u0026lt;- FALSE # create note and check it { fit \u0026lt;- lm(y ~ ., df_in) aov \u0026lt;- anova(fit) # Create the vector to save names, F-values, P-values C_names \u0026lt;- colnames(df_in) F_value \u0026lt;- aov[-(in_width+1),4] P_value \u0026lt;- aov[-(in_width+1),5] note \u0026lt;- data.frame(name=C_names, F_value=F_value, P_value=P_value) # Find whos F-value is smallst and take it P-value min_F \u0026lt;- min(note$F_value) min_Fvar \u0026lt;- which(note$F_value==min_F) alpha_test \u0026lt;- as.numeric(note$P_value[min_Fvar]) } # if it pass the check, add in and drop out { # Drop it out to model if(alpha_test \u0026gt; alpha_in){ # if the biggest F-value variable can be catch, we set countinue = TRUE continue \u0026lt;- TRUE # get the biggest F-value variable who_catch \u0026lt;- note$name[min_Fvar] drop_var \u0026lt;- which(colnames(df_in)==who_catch) drop_data \u0026lt;- as.data.frame(df_in[, drop_var]) colnames(drop_data) \u0026lt;- who_catch # find it and add it from the df_out data.frame if(out_width!=0){ df_out \u0026lt;- cbind(drop_data, df_out) }else{ df_out \u0026lt;- as.data.frame(drop_data) colnames(df_out) \u0026lt;- who_catch } # find who into the df_in, and delete it from df_out if(in_width!=1){ df_in[, who_catch] \u0026lt;- c() }else{ df_in \u0026lt;- data.frame() } # since one variable move to df_in from df_out, change the width of df out_width \u0026lt;- out_width+1 in_width \u0026lt;- in_width -1 cat(\u0026#39;in:\u0026#39;, in_width, \u0026#39;/ out:\u0026#39;, out_width, \u0026#39;:\u0026#39;, who_catch, \u0026#39;(OUT) \\n\u0026#39;) } } who_out \u0026lt;- who_catch if(who_in==who_out | in_width){ break_count \u0026lt;- break_count + 1 }else{ break_count \u0026lt;- 0 } if(break_count \u0026gt; early_break_count){ message_var \u0026lt;- paste(\u0026#34;Break warning by variable in=out\u0026#34;) warning(message_var) break } # stepwise part start # check df_out variable \u0026gt; 1 if(out_width \u0026gt; 1){ # delete this loop new variable if(out_width!=2){ step_df \u0026lt;- df_out[,-1] }else{ save_name \u0026lt;- colnames(df_out)[2] step_df \u0026lt;- as.data.frame(df_out[,-1]) colnames(step_df) \u0026lt;- save_name } # create information of df_out model to \u0026#34;backward_note\u0026#34; forward_fit \u0026lt;- lm(y ~ ., step_df) # use another ones to anova forward_aov \u0026lt;- anova(forward_fit) C_names \u0026lt;- colnames(step_df) F_value \u0026lt;- forward_aov$`F value`[1:out_width-1] # out_width-1, since we delete this loop new variable P_value \u0026lt;- forward_aov$`Pr(\u0026gt;F)`[1:out_width-1] # out_width-1, since we delete this loop new variable backward_note \u0026lt;- data.frame(name=C_names, F_value=F_value, P_value=P_value) # Find whos F-value is biggest and take it P-value max_F \u0026lt;- max(backward_note$F_value) max_Fvar \u0026lt;- which(backward_note$F_value==max_F) alpha_test \u0026lt;- as.numeric(backward_note$P_value[max_Fvar]) if(alpha_test \u0026lt;= alpha_out){ # if the biggest F-value variable can be catch, we set countinue = TRUE continue \u0026lt;- TRUE # get the smallest F-value variable who_catch \u0026lt;- backward_note$name[max_Fvar] catch_var \u0026lt;- which(colnames(df_out)==who_catch) catch_data \u0026lt;- as.data.frame(df_out[, catch_var]) colnames(catch_data) \u0026lt;- who_catch # find it and add it to the df_in data.frame if(in_width!=0){ df_in \u0026lt;- cbind(catch_data, df_in) }else{ df_in \u0026lt;- catch_data } # find who into the df_in, and delete it from df_out df_out[, catch_var] \u0026lt;- NULL # since one variable move to df_in from df_out, change the width of df out_width \u0026lt;- out_width-1 in_width \u0026lt;- in_width +1 cat(\u0026#39;in:\u0026#39;, in_width, \u0026#39;/ out:\u0026#39;, out_width, \u0026#39;:\u0026#39;, who_catch, \u0026#39;(IN) \\n\u0026#39;) } } who_in \u0026lt;- who_catch # stepwise part end # if no any variable in df_out, stop loop if(out_width==0){ break } } } # vif check part { fit \u0026lt;- lm(y ~ ., df_in) vif_value \u0026lt;- as.data.frame(car::vif(fit)) who_catch \u0026lt;- vif_value[vif_value[,1]==max(vif_value),] name \u0026lt;- rownames(vif_value)[which(vif_value[,1]==who_catch)] if(vif_value[which(vif_value[,1]==who_catch),1]\u0026gt;=10){ message_vif \u0026lt;- paste(name, \u0026#34;have \u0026#39;Severe\u0026#39; Multicollinearity problem\u0026#34;) warning(message_vif) }else if(vif_value[which(vif_value[,1]==who_catch),1]\u0026gt;=5){ message_vif \u0026lt;- paste(name, \u0026#34;maybe have Multicollinearity problem\u0026#34;) warning(message_vif) }else{ print(\u0026#34;Vif test pass!\u0026#34;) } } return(fit) } ","date":"9 June 2024","externalUrl":null,"permalink":"/posts/r_stepwise/","section":"Posts","summary":"以 R 語言實作基於 F-value 的 Forward 及 Backward Stepwise Model Selection，並將其製作成函數及 package","title":"基於 F-value 的 Stepwise Model Selection","type":"posts"},{"content":"","date":"3 June 2024","externalUrl":null,"permalink":"/tags/mysql/","section":"Tags","summary":"","title":"Mysql","type":"tags"},{"content":"","date":"3 June 2024","externalUrl":null,"permalink":"/tags/sql/","section":"Tags","summary":"","title":"Sql","type":"tags"},{"content":"","date":"3 June 2024","externalUrl":null,"permalink":"/series/sqlzoo/","section":"Series","summary":"","title":"SQLZOO","type":"series"},{"content":" About Data # 這是SQLZOO 學習紀錄系列的第二篇，這篇文章對應的章節是More JOIN operations 。我會在下方說明我的解題過程。讓我們來看到這次所使用的資料表：\nmovie\nid title yr director budget gross 10003 Crocodile Dundee II 1988 38 15800000 239606210 10004 Til There Was You 1997 49 10000000 \u0026hellip; actor\nid name 20 Paul Hogan 50 Jeanne Tripplehorn \u0026hellip; casting\nmovieid actorid ord 10003 20 4 10004 50 1 \u0026hellip; Introduction # 在這個章節中，我們會基於上個章節的內容做更多合併與檢索資料的練習。下面是這個章節中例題 的解答，同時我也會解說部分例題的解題思路：\nQuestion 1\nSELECT id, title FROM movie WHERE yr=1962; Question 2\nSELECT yr FROM movie WHERE title= \u0026#39;Citizen Kane\u0026#39;; Question 3\nSELECT id, title, yr FROM movie WHERE title LIKE \u0026#39;%Star Trek%\u0026#39; ORDER BY yr; Question 4\nSELECT id FROM actor WHERE name=\u0026#39;Glenn Close\u0026#39;; Question 5\nSELECT id FROM movie WHERE title=\u0026#39;Casablanca\u0026#39;; Question 6\nSELECT name FROM casting JOIN actor ON actorid=id WHERE movieid=( SELECT id FROM movie WHERE title=\u0026#39;Casablanca\u0026#39;); Question 7\nSELECT name FROM casting JOIN actor ON actorid=id WHERE movieid=( SELECT id FROM movie WHERE title=\u0026#39;Alien\u0026#39;); Question 8\nSELECT title FROM movie JOIN actor ON movie.id=actor.id JOIN casting ON movie.id=movieid WHERE actorid=( SELECT id FROM actor WHERE name=\u0026#39;Harrison Ford\u0026#39;); Question 9\nSELECT title FROM casting JOIN movie ON movieid=movie.id JOIN actor ON actorid=actor.id WHERE name=\u0026#39;Harrison Ford\u0026#39; and ord!=1; Question 10\nSELECT title, name FROM casting JOIN movie ON movieid=movie.id JOIN actor ON actorid=actor.id WHERE yr=1962 AND ord=1; Harder Questions # Question 11\nSELECT yr,COUNT(title) FROM casting JOIN movie ON movieid=movie.id JOIN actor ON actorid=actor.id WHERE name=\u0026#39;Rock Hudson\u0026#39; GROUP BY yr HAVING COUNT(title) \u0026gt; 2; Question 12\n在這題中我原本使用 title 去做搜尋，但似乎有一部名為 \u0026ldquo;Little Miss Marker twice\u0026rdquo; 的電影有兩位主演，導致若是以名字去做搜尋會得到兩份結果，導致在網站中答案顯示不正確，但我覺得題目中並沒有提到當電影有兩位主演時只能顯示一位，因此我在這裡附上兩種答案，第一份是符合網站要求的標準答案，而第二份我同時顯示了 ord 來確認在這部電影中真的有兩位主演。\n-- 標準答案 SELECT title, name FROM casting JOIN movie ON movieid=movie.id JOIN actor ON actorid=actor.id WHERE movie.id IN ( SELECT movie.id FROM casting JOIN movie ON movieid=movie.id JOIN actor ON actorid=actor.id WHERE name=\u0026#39;Julie Andrews\u0026#39;) AND ord=1; -- 若是以電影名稱下去檢索會出現回答錯誤 SELECT title, name, ord -- 可移除ord來顯示與標準答案格式相同的答案 FROM casting JOIN movie ON movieid=movie.id JOIN actor ON actorid=actor.id WHERE title IN ( SELECT title FROM casting JOIN movie ON movieid=movie.id JOIN actor ON actorid=actor.id WHERE name=\u0026#39;Julie Andrews\u0026#39;) AND ord=1; Question 13\n在這題中，我們需要輸出不重複的演員，同時這須演員必須至少要出演15部電影的主演。 因此在這裡我們使用DISTINCT確保輸出的演員名字不重複，在合併資料表後，我們使用IN來選擇符合要求的演員id，由WHERE選取主演的資料後，使用GROUP BY按照演員名字分組，最後使用HAVING判斷由COUNT統計的出現次數超過15次的演員名單後回傳。\nSELECT DISTINCT name FROM casting JOIN movie ON movie.id=movieid JOIN actor ON actor.id=actorid WHERE actorid IN ( SELECT actorid FROM casting WHERE ord=1 GROUP BY actorid HAVING COUNT(actorid) \u0026gt;= 15) ORDER BY name; Question 14\nSELECT title, COUNT(actorid) FROM casting JOIN movie ON movie.id=movieid WHERE yr=1978 GROUP BY movieid, title ORDER BY COUNT(actorid) DESC, title; Question 15\n在最後一題中，我們需要找出與 \u0026ldquo;Art Garfunkel\u0026rdquo; 一起出演過電影的演員，在這裡我們需要由電影搜尋哪些演員曾在他出演過的電影中出現，同時我們需要加上一個不能為他本人的條件確保輸出的結果正確。\nSELECT DISTINCT name FROM casting JOIN actor ON actorid = actor.id WHERE movieid IN ( SELECT movieid FROM casting JOIN movie ON movie.id=movieid JOIN actor ON actor.id=actorid WHERE name = \u0026#39;Art Garfunkel\u0026#39;) AND name != \u0026#39;Art Garfunkel\u0026#39;; ","date":"3 June 2024","externalUrl":null,"permalink":"/posts/sql_sqlzoo_morejoinans/","section":"Posts","summary":"在 SQLZOO More JOIN 章節練習時的習題實作及詳解","title":"SQLZOO解答與討論（More JOIN）","type":"posts"},{"content":" About Data # 這裡是我在 SQLZOO 這個網站的學習紀錄，這篇文章對應的章節是The Join operation 。我會在下方說明我的解題過程。首先讓我們來看到這個章節所使用的資料表：\ngame\nid mdate stadium team1 team2 1001 8 June 2012 National Stadium, Warsaw POL GRE 1002 8 June 2012 Stadion Miejski (Wroclaw) RUS CZE 1003 12 June 2012 Stadion Miejski (Wroclaw) GRE CZE 1004 12 June 2012 National Stadium, Warsaw POL RUS \u0026hellip; goal\nmatchid teamid player gtime 1001 POL Robert Lewandowski 17 1001 GRE Dimitris Salpingidis 51 1002 RUS Alan Dzagoev 15 1002 RUS Roman Pavlyuchenko 82 \u0026hellip; eteam\nid teamname coach POL Poland Franciszek Smuda RUS Russia Dick Advocaat CZE Czech Republic Michal Bilek GRE Greece Fernando Santos \u0026hellip; 上面的資料在SQLZOO有下載連結 ，表格中包含波蘭和烏克蘭舉行的 2012 年歐洲足球錦標賽的所有比賽和進球。\nIntroduction # 在這個章節中，我們可以學到在調用多個資料庫時合併資料庫查詢資料的方法，假設我們分別有兩個資料表為table1及table2，若我們想把它們根據各自的id合併並輸出合併後全部的資料，那麼我們可以用下面的語法來做到。\nSELECT * FROM table1 JOIN table2 ON id1=id2 下面是這個章節中例題 的解答，同時我也會解說部分例題的解題思路：\nQuestion 1\nSELECT matchid, player FROM goal WHERE teamid=\u0026#39;GER\u0026#39;; Question 2\nSELECT id,stadium,team1,team2 FROM game WHERE id=1012; Question 3\nSELECT player, teamid, stadium, mdate FROM game JOIN goal ON (id=matchid) WHERE teamid=\u0026#39;GER\u0026#39;; Question 4\n在這裡我們有使用到前面章節所提到的正則表達式，若我們想找出名字前綴為\u0026rsquo;Mario’的球員，我們可以用 WHERE player LIKE \u0026lsquo;Mario%\u0026rsquo; 來做查詢。\nSELECT team1, team2, player FROM game JOIN goal ON (id=matchid) WHERE player LIKE \u0026#39;Mario%\u0026#39;; Question 5\nSELECT player, teamid, coach, gtime FROM goal JOIN eteam ON (id=teamid) WHERE gtime\u0026lt;=10; Question 6\nSELECT mdate, teamname FROM game JOIN eteam ON (team1=eteam.id) WHERE coach=\u0026#39;Fernando Santos\u0026#39;; Question 7\nSELECT player FROM game JOIN goal ON (id=matchid) WHERE stadium=\u0026#39;National Stadium, Warsaw\u0026#39;; Question 8\n在這題中，我們想要知道有與德國隊比賽的隊伍中，哪些球員曾經將球踢進了德國隊的球門，由於同一名球員可能曾經進球超過一次，因此我們在輸出時使用 DISTINCT(player) 來將這些球員的名字取不重複的名單出來。\nSELECT DISTINCT(player) FROM game JOIN goal ON (id=matchid) WHERE (team1=\u0026#39;GER\u0026#39; OR team2=\u0026#39;GER\u0026#39;) AND teamid != \u0026#39;GER\u0026#39;; Question 9\n由於這裡我們想統計個別球隊的球隊名稱與其進球數，與上一題不同的是，我們這裡可以用 GROUP BY teamname 來將各個輸出結果以球隊分類，同時我們在輸出時也使用 COUNT(player) 來統計每一隊進球的總數。\nSELECT teamname, COUNT(player) FROM eteam JOIN goal ON (id=teamid) GROUP BY teamname; Question 10\nSELECT stadium, COUNT(player) FROM game JOIN goal ON (id=matchid) GROUP BY stadium; Question 11\nSELECT matchid, mdate, COUNT(player) FROM game JOIN goal ON id=matchid WHERE team1 = \u0026#39;POL\u0026#39; OR team2 = \u0026#39;POL\u0026#39; GROUP BY id, mdate; Question 12\nSELECT matchid, mdate, COUNT(player) FROM game JOIN goal ON id=matchid WHERE teamid=\u0026#39;GER\u0026#39; GROUP BY id, mdate; Question 13\n在最後一題中，我們使用到了CASE這個語法，它的目的是將原始輸出的結果透過條件式來更改成指定的輸出，類似於將Ｒ或Python中的if-else條件判斷式嵌入其中；由於題目要求的是比賽雙方的得分數，因此我們在使用SUM將更改後的輸出做加總；同時為了更符合題目的要求，我們將輸出的表格名稱定義為score1及score2，最後我們再依照mdate、id、team1、team的順序依序做分群及排列。\nSELECT mdate, team1, SUM(CASE WHEN teamid = team1 THEN 1 ELSE 0 END) AS score1, team2, SUM(CASE WHEN teamid = team2 THEN 1 ELSE 0 END) AS score2 FROM game LEFT JOIN goal ON id=matchid GROUP BY mdate, id, team1, team2 ORDER BY mdate, id, team1, team2 ","date":"30 May 2024","externalUrl":null,"permalink":"/posts/sql_-sqlzoo_joinans/","section":"Posts","summary":"在 SQLZOO JOIN 章節練習時的習題實作及詳解","title":"SQLZOO解答與討論（JOIN）","type":"posts"},{"content":" 自我介紹 # 我是施承翰，我於 2023 年 6 月取得國立東華大學應用數學系的統計碩士學位，並於 2024 年的 3 月底正式服完義務兵役。\n在我的學習歷程中，我發現我已深深愛上了資料科學，因為每次數據分析都為我提供了一個看待世界的新視角，同時我也對探討數據中的細節樂此不疲，因此我平時在數據科學領域上總是保有好奇心，積極研究和掌握與現今追求職務相關的知識。\n我於在學期間接觸過許多數據分析的理論及應用，諸如線型迴歸模型與各類機器學習模型建模、資料視覺化及變數的特徵工程等等。就學期間，我擅長使用資料分析及模型建構參與團隊合作，從大量資料中挖掘出有價值且實用的見解，並應用於決策與報告的呈現。\n在網站中我除了會更新對不同數據所進行的分析報告外，我也會在網站中的個人部落格更新我過去所學及對新知識的個人心得。我想以此紀錄下我在這條路途上的一步一腳印，同時也督促自己人生的旅途才剛起步，我還要更加精進自己已走向更遠的路途。\n學經歷 # 國立東華大學 應用數學系 統計組 碩士 2021 / 09 ~ 2023 / 06\n國立東華大學 應用數學系 統計組 學士 2017 / 09 ~ 2021 / 06\n國立桃園高中 普通科 2014 / 09 ~ 2017 / 06\n第32屆南區統計研討會 碩士論文短講 演講者 2023 / 06\n演講題目為 BGG 桌遊圖，目標是將桌上遊戲的相關數據抓取後，基於文字探勘與模型建構後呈現遊戲之間的關係，並且將成果架設至網頁 上提供客製化的模型輸出，我也將其簡單的說明報告 收錄在本網站中。 第32屆南區統計研討會 攝影企劃團隊 負責人 2023 / 06\n負責活動期間的攝影及成果的彙整。規劃與安排研討會期間各演講及活動進行的攝影人員，並與其他團隊溝通進行人力調動。 國立東華大學 教學助理 2022 / 02 ~ 2023 / 06\n負責科目為Ｒ軟體實作、統計學與微積分，工作內容為解答學生課程疑惑、主持演習課程與協助教授處理課程相關事務。 2022 AI \u0026amp; Data Science Workshop 工作人員 2022 / 01\n擔任活動第三會場負責人，負責現場活動流程運作及講者接洽，並與其他團隊合作處理突發狀況。 專長 # 程式語言：R、Python、SQL\n資料分析與視覺化\n機器學習建模\n文字探勘 / 特徵工程\n統計實務應用 / 模型解釋\n證照 # IPAS 初級巨量資料分析師 聯絡方式 # Gmail : hans880803@gmail.com 個人簡歷下載連結 ","date":"17 May 2024","externalUrl":null,"permalink":"/about/","section":"Hans Website","summary":"\u003ch2 class=\"relative group\"\u003e自我介紹 \n    \u003cdiv id=\"自我介紹\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#%e8%87%aa%e6%88%91%e4%bb%8b%e7%b4%b9\" aria-label=\"定位點\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e我是施承翰，我於 2023 年 6 月取得國立東華大學應用數學系的統計碩士學位，並於 2024 年的 3 月底正式服完義務兵役。\u003c/p\u003e","title":"About","type":"page"},{"content":"","date":"20 April 2024","externalUrl":null,"permalink":"/tags/api/","section":"Tags","summary":"","title":"Api","type":"tags"},{"content":"","date":"20 April 2024","externalUrl":null,"permalink":"/tags/visualization/","section":"Tags","summary":"","title":"Visualization","type":"tags"},{"content":"這段程式碼是使用 R 語言的 ggmap 和 ggforce 函式庫來繪製地圖並在地圖上標註指定位置的周圍半徑範圍內的圓形區域。以下是程式碼的主要步驟：\n設定 Package、Google Maps API 金鑰： 首先我們載入 ggmap 使用 register_google 函數設定 Google Maps API 金鑰，這是為了能夠使用 Google Maps 的地圖服務。而 ggforce 是為了調用 Google Maps 來進行後續的繪圖使用。 library(ggmap) library(ggforce) # 設定 Google Maps API 金鑰 register_google(key = apikey) 讀取數據： 從指定的 CSV 文件中讀取地名列表，該文件包含站序和站名，需注意的是必須要將格式指定為 UTF-8，否則有時會發生無法正常讀取中文字符串的情況。 station \u0026lt;- read.csv(\u0026#34;/Users/hans/Documents/taroko_bus/data/station.csv\u0026#34;, encoding = \u0026#34;UTF-8\u0026#34;) 地理編碼： 使用 geocode 函數將地名列表中的站名轉換為地理座標（經度和緯度）。但這邊我們因為無法一次就抓到正確位址所以透過加入關鍵字的方式來輔助 Google Maps 搜尋，同時剔除搜尋錯誤如範圍在台灣之外的結果再重新搜尋一次。 # 要轉換的地名列表 locations \u0026lt;- data.frame(count=station$站序8181[1:38], station=station$站名8181[1:38]) taitung_add \u0026lt;- \u0026#34;taitung, taitung city, taitung county, taiwan 950\u0026#34; words \u0026lt;- c(\u0026#34;\u0026#34;, \u0026#34;公車站\u0026#34;, \u0026#34;站\u0026#34;, \u0026#34;車站\u0026#34;, \u0026#34;花蓮\u0026#34;, \u0026#34;台東\u0026#34;) location_row \u0026lt;- 1:dim(locations)[1] geocodeds \u0026lt;- data.frame() for(word in words){ geocoded \u0026lt;- geocode(paste0(locations$station, word), output = \u0026#34;more\u0026#34;, source = \u0026#34;google\u0026#34;) geocoded \u0026lt;- cbind(location_row, geocoded) if(word==\u0026#34;花蓮\u0026#34;){ inspace \u0026lt;- which(geocoded$location_row\u0026gt;9) geocoded$address[inspace] \u0026lt;- NA }else if(word==\u0026#34;台東\u0026#34;){ geocoded$address[which(geocoded$address==taitung_add)] \u0026lt;- NA } locations \u0026lt;- locations[which(is.na(geocoded$address)==T | geocoded$lon\u0026gt;=125 | geocoded$lon\u0026lt;=120 | geocoded$lat\u0026gt;=25 | geocoded$lat\u0026lt;=21),] geocoded \u0026lt;- geocoded[which(is.na(geocoded$address)==F \u0026amp; geocoded$lon\u0026lt;=125 \u0026amp; geocoded$lon\u0026gt;=120 \u0026amp; geocoded$lat\u0026lt;=25 \u0026amp; geocoded$lat\u0026gt;=21),] geocodeds \u0026lt;- rbind(geocodeds, geocoded) location_row \u0026lt;- locations$count } 計算圓形坐標點： 根據指定的中心點經緯度和半徑，計算圓形的坐標點。 # 指定中心點經緯度和半徑 center \u0026lt;- data.frame(lon = geocodeds$lon, lat = geocodeds$lat) # 中心點的經緯度 radius_km \u0026lt;- 1 # 半徑，單位為公里 # 將半徑從公里轉換為度 radius_deg \u0026lt;- radius_km / 111.32 # 每緯度大約111.32公里 繪製地圖： 使用 get_map 函數取得地圖圖像，然後使用 ggmap 函數將地圖加載到 output 中。接著，使用 geom_point 和 geom_path 函數在地圖上標註中心點和圓形區域，最後將結果顯示在地圖上。 # 使用 get_map 函數取得地圖圖像 map \u0026lt;- get_map(location = c(center$lon[10], center$lat[10]), zoom = 11) output \u0026lt;- ggmap(map) for(i in 1:nrow(geocodeds)){# 計算圓形的坐標點 circle_points \u0026lt;- data.frame( lon = center$lon[i] + radius_deg * cos(seq(0, 2*pi, length.out = 100)), lat = center$lat[i] + radius_deg * sin(seq(0, 2*pi, length.out = 100)) ) # 繪製地圖 output \u0026lt;- output + geom_point(x = center$lon[i], y = center$lat[i], size = 3, color = \u0026#39;red\u0026#39;) + geom_path(data = circle_points, aes(x = lon, y = lat), color = \u0026#34;red\u0026#34;, size = 1) } output 下面是最後顯示在地圖上的結果，紅點為公車站所在的位址，而外圈則為標記半徑一公里的地區。\n","date":"20 April 2024","externalUrl":null,"permalink":"/posts/r_ggmap/","section":"Posts","summary":"使用 R 語言調用 Google Map Api 來繪製附加公車站牌資訊的地圖","title":"繪製基於 GGMap 的公車路線圖","type":"posts"},{"content":"桌上遊戲是近年來迅速成長的產業之一，由於近幾年疫情的影響，人們參與大型戶外活動的意願相較疫情前有所降低，相較於在外接觸陌生人，這時待在家中或是與熟識的少數朋友小聚成為許多人的選擇，進而意外促進了許多產業的蓬勃發展，而桌遊便是其中之一。\n由於我個人本身對桌遊也抱持著濃厚的興趣，因此我的碩士論文便以桌上遊戲的視覺化為主題，希望能在明確的空間定義下找出相似的遊戲，而我的研究資料來自於BGG 這個網站，它是全球最大的桌遊論壇，上面刊載的桌遊多達數十萬筆，我透過網路爬蟲從中獲取數據。\n在這篇文章中，由於篇幅有限，我主要的介紹目標是呈現我的視覺化成果，因此若是需要更為詳細的資訊，歡迎查閱我碩士論文 的內容或是與我聯繫。\nA little bit EDA # 在呈現視覺化的篇幅之前，我先為快速為接下來出現的幾張圖所使用到的變數做一些簡單的說明。\nGametype：在這裡我們把它稱為遊戲屬種，一款遊戲的屬種是由BGG上的用戶投票決定的，一款遊戲會有 1 到 3 個屬種，而屬種的類型一共有 8 個，它們分別是抽象策略遊戲、主題遊戲、家庭遊戲、兒童遊戲、派對遊戲、策略遊戲、戰爭遊戲及可定制策略遊戲，它們分別描述了不同的遊戲。\nCategory：在這裡我們把它稱為遊戲類別，遊戲類別的部分變數數量相對較多，共分為 83 個，因此在這裡我們便不一一介紹，遊戲類別代表著遊戲內有使用到 的元素，例如戰爭、卡牌、科幻、經濟和戰鬥等等。在一款遊戲中的遊戲的類別通常不只一個，在這筆資料裡一款遊戲甚至可以達到 14 個類別。\nMechanic：在這裡我們把它稱為遊戲機制，遊戲機制是一個多達 182 種機制的類別變數，由於遊戲機制眾多，而大多數的機制都不是太過熱門，少數如擲骰子、棋盤遊戲、手牌管理、收集要素等等較為熱門，而一款遊戲的機制數目更是可以來到遊戲類別的兩倍不止。\n透過這其中幾個變數的說明我們可以發現它們是一個複雜的 multivalue 問題，並且同時因為資料十分稀疏，因此若是未經處理便直接對其進行分析，那麼將會造成這個稀疏矩陣所擁有的資訊量並不足以釐清變數之間的關係，那麼我們便無法從中獲得足夠的資訊。\nData Visualization # 在這裡我主要將呈現兩個部份的視覺化結果，它們分別對應到遊戲類別與各遊戲之間的關係，首先讓我們看到遊戲類別的部分。\nCategory # 在遊戲類別的視覺化中，我將矩陣做了轉制處理，因為我好奇遊戲類別之間的關係，同時我們懷疑在不同的遊戲屬種上可以看到有不同的類別取向，例如客群面向家庭及兒童的遊戲所擁有的類別標籤，與注重策略的遊戲會擁有不同的類別標籤，嘗試了許多方法後，我們最後採用 Jaccard 計算較熱門類別之間的距離後，利用 PCA 對遊戲類別做資料視覺化的呈現，而PC1 與 PC2 的解釋變異也多達 17%。\n在第一張圖中，我們透過兩個算法的相互配合，讓原本難以被定義距離的類別之間可以在二維空間有很好的呈現。\n備註：在圖中抽象策略、戰爭、兒童的遊戲類別標籤與遊戲屬種不同，擁有特定類別的標籤不代表就是該屬種的遊戲。遊戲類別並不能如同遊戲屬種一樣提供使用者投票，因此可能較符合原始遊戲設計師的類別定義。\n在這張圖中我們可以看到各遊戲類別之間的關係，從對桌遊類別語意的直接解讀而言，在圖的上半部分多為戰爭相關的類別，中間部分的策略成分較高一些，左下角為一些帶有主題例如冒險、奇幻類的遊戲，右下部分看似表現不好誤將兒童與恐怖等遊戲混雜在一起，但這只是因為在二維空間下他們的距離無法被拉開所導致，如果我們從 PC2 及 PC3 的角度來觀察結果，便可以將兒童與恐怖遊戲分開，它們分別位於下圖中的左上方與左下方。\n但到這裡為止並不是完整的呈現，我們在這裡使用遊戲屬種為遊戲類別上色。同時我們提取所有屬於該類別的遊戲，接著將它們的第一個遊戲屬種之於全體遊戲屬種之間的比例進行權重調整，最後取最占比最高的屬種來決定這一個遊戲類別是屬於什麼遊戲屬種，目的是避免上色時存在遊戲屬種數量優勢的問題。\n我們可以看到上色後的結果如同我們猜測一樣，不同的遊戲屬種上可以看到有不同的類別取向，同時他們可以被很好的區分及視覺化。\nGames # 接著我們下一步將對遊戲之間做視覺化的呈現，透過加入如遊戲機制、複雜度等等其他處理後的變數，更甚至是遊戲屬種也做權重調整後加入，因為我們的目標並不是將遊戲分群，而是將遊戲之間的關係釐清做視覺化呈現。我們可以看到遊戲的四張桌遊地圖，它們分別代表我對不同需求所構建的視覺化成果。\nOrginal # 這是一張原始版本的桌遊地圖，它對應的是沒有對運算過程做權重調整的輸出，相較於其他版本，這張地圖的受眾可以是所有想了解桌遊之間關係的人，透過可以被解釋的距離，我們可以很好的看出遊戲之間的關係，因此在呈現上除了最左邊的抽象策略屬種以外，其他的屬種彼此之間並沒有相互拉開距離，也很好的反映出現代的桌遊其實大多與過去如圍棋、象棋和西洋棋等抽象策略遊戲已經有所不同。\nCategory # 相較前者而言，在這張桌遊地圖中，我們增加了遊戲類別的權重，調整後的結果使得相近遊戲類別的遊戲更加接近了，同時也稍微拉近了一點相同屬種的遊戲之間的距離。如果目標是找尋一款遊戲畫風或是主題相近的遊戲，那麼將遊戲類別的權重加重可能是一個不錯的選擇。\nMechanic # 接著是關於較注重遊戲機制的桌遊地圖，遊戲機制的權重加重可以讓我們更好的找到玩法相近的遊戲。但由於不同屬種之間的遊戲在遊戲機制的區分上並不明顯，因此在加重機制權重佔比的地圖中我們發現在各桌遊屬種上的分佈更加混雜，這張地題對策略遊戲等較注重機制玩法類的遊戲可能會有較好的效果，目標受眾是想尋找玩法相近的遊戲。\nGametype # 最後是遊戲屬種的桌遊地圖，透過加入相對較高額的遊戲屬種權重讓不同屬種的遊戲彼此分開，但又不過分將其他資訊稀釋而讓遊戲屬種之間不至於完全分開，在搜索同屬種的相近遊戲上可以有不錯的表現，適合目標為尋找同屬種遊戲的人群，例如找尋彼此相近的派對遊戲等。\nWebsite Demo # 在完成資料視覺化之後，我們將桌遊地圖透過架設至網頁 上讓使用者可以進行實時的互動，我們可以從第一張圖片中的搜索欄位輸入遊戲的名稱，按下繪製結果後便可得到該遊戲在選定權重種類的局部放大圖及全圖，並可以透過滑鼠與圖片進行互動。使用者可自行選擇四種權重來查看不同的結果。而下方的表格為以歐式距離計算，距離目標遊戲最近的 30 個遊戲。\n首先我們先查看原始權重的桌遊地圖實例，讓我們以 Azul 這個遊戲為例，這是一款透過拼湊花磚在遊戲中比較得分的桌遊，我們可以看到除了遊戲本身的其他版本以外，其餘的遊戲許多都具有拼圖的遊戲元素存在，並且它們多有相近的遊戲機制成分。\n接著我們查看遊戲類別權重加重的桌遊地圖實例，接著我們以 Carcassonne 這個遊戲為例，我們扮演了想尋找相近遊戲元素的使用者，我們可以從圖中看到在下方的表格中，許多遊戲都具有中世紀、城堡、城市建造等遊戲元素存在，同時也存在同款遊戲的其他版本。\n我們以 The Resistance: Avalonl，也就是大家所熟知的阿瓦隆來查看遊戲機制權重加重的桌遊地圖實例，我們從想尋找溝通推理的用戶角度出發，在下圖的下方表格中，許多需要玩家之間溝通推理等的遊戲可以被很好的看到。\n最後我們以 Agricola，在台灣名稱為農家樂，同時也是台灣早期被稱為「神農電波」四大策略桌遊之一的知名遊戲來查看遊戲屬種權重加重的桌遊地圖實例，在這裡我們扮演的是想尋找重度策略遊戲的使用者，我們可以從下圖中的表格，我們可以看到 Terra Mystica（神秘大地）、Power Grid （電力公司）、Puerto Rico（波多黎各）等遊戲均在表中。\nConclusion # 透過不同的視覺化方法對桌遊資料做解析後，使我們對遊戲之間的關係有更深入的認識。增加遊戲類別的權重將加強輸出結果之間遊戲主題的相關性，同時將遊戲屬種之間的距離做些許的拉近。而遊戲機制會使輸出的結果更加偏向遊戲性，而遊戲屬種之間亦會相互混雜。它們兩者對將桌遊地圖視覺化缺一不可，僅透過類別或機制均無法對遊戲整體的相似做較好的還原。最後我們透過加入些許遊戲屬種與其他變數的資訊可以讓目標客群相近的遊戲可以更容易被互相吸引，但遊戲屬種的權重過高會造成不同群體的遊戲之間無法互相被看到，因此調整適當的權重是十分重要的。\n","date":"30 June 2023","externalUrl":null,"permalink":"/projects/master_bgg/","section":"Projects","summary":"透過抓取 BoardGameGeek 桌遊網站的遊戲資料後，將不同桌遊彼此之間距離進行可解釋的資料視覺化","title":"BGG 桌遊地圖","type":"projects"},{"content":"","date":"30 June 2023","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"},{"content":"","date":"5 June 2023","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine-Learning","type":"tags"},{"content":" 藍莓因其豐富的營養成分常被譽為超級食物，因此引起了人們對其健康益處和烹飪多樣性的高度關注。在各種藍莓品種中，野生藍莓以其高含量的維生素、錳、花青素、類胡蘿蔔素、鉀、鋅和抗氧化劑而脫穎而出。值得注意的是，與栽培藍莓相比，野生藍莓的抗氧化劑含量顯著更高，使其成為農業市場上一種備受追捧的商品。\n為了最大程度地提高野生藍莓的產量，研究人員和種植者都在尋求創新的方法來預測和優化生產成果。其中一種途徑是利用預測建模技術，基於從田間觀察中得出的一系列豐富的預測變量來預測野生藍莓的產量。這些變量涵蓋了影響野生藍莓生長和生產力的各種因素。\nInduction # 在這項分析中，我們旨在利用從Kaggle競賽中提供的野生藍莓產量預測數據集中獲取的數據，開發一個用於估算野生藍莓產量的預測模型。我們的分析將集中於整合各種預測變量，包括環境因素、授粉者密度和植物特徵，以建立一個穩健的預測模型。\n讓我們看看數據集中有哪些變量。\nClonesize ( m2 ) : 每平方公尺中有多少比例的藍莓是經由無性生殖所培育出來的。\nHoneybee ( m2 / min ) : 田間每平方公尺蜜蜂的密度。\nBumbles ( m2 / min ) : 田間每平方公尺大黃蜂的密度。\nAndrena ( m2 / min ) : 田間每平方公尺安德瑞那蜂的密度。\nOsmia ( m2 / min ) : 田間每平方公尺奧斯米亞蜂的密度。\nMax.U.T ( Max of Upper TRange ◦F ) : 在整個開花季節期間，每日上限氣溫的最高紀錄。\nMin.U.T ( Min of Upper TRange ◦F ) : 在整個開花季節期間，每日上限氣溫的最低紀錄。\nA.U.T ( Average of Upper TRange ◦F ) : 在整個開花季節期間，每日上限氣溫的平均值。\nMax.L.T ( Max of Lower TRange ◦F ) : 在整個開花季節期間，每日下限氣溫的最高紀錄。\nMin.L.T ( Min of Lower TRange ◦F ) : 在整個開花季節期間，每日下限氣溫的最低紀錄。\nA.L.T ( Average of Lower TRange ◦F ) : 在整個開花季節期間，每日下限氣溫的平均值。\nR.D ( Raining Days) : 在整個開花季節期間，降雨量大於零的總天數。\nA.R.D ( Average Raining Days) : 在整個開花季節期間，降雨天數的平均值。\nFruitset : 最終結果形成果實的花朵比例。\nFruitmass ( g ) : 單個藍莓果實的平均質量（克）。\nSeeds : 平均每個藍莓果實中發現的種子總數。\n反應變量 : Yield（產量）\nFigure 1: Data Table 1 Figure 2: Data Table 2 Figure 3: Data Table 3 Exploratory Data Analysis, EDA # Correlation # 根據對數據集的初步分析，我們從 Figure 4: Correlation Plot 中，發現右下角的 Fruitset、Fruitmass、Seeds 和 Yield 彼此之間幾乎完全呈現正相關；我們可以預期它們之間密切相關，因為更高的結果比例提升了產量基數，而單個藍莓果實的平均質量更是如此。\n而大黃蜂和奧斯米亞蜂也與 Fruitset、Fruitmass、Seeds 和 Yield 呈正相關，我們推測這可能是因為蜜蜂授粉對藍莓而言是結果的必要因素，因此更高的田間蜜蜂密度可以帶來更好的結果率，進而影響產量。\n此外，無性生殖比例大小與Fruit-set、Fruitmass、Seeds、Yield呈負相關。經過了解，我們發現這是因為有性繁殖的後代生產量將高於無性繁殖。\n降雨天數也與 Fruitset、Fruitmass、Seeds、Yield 呈負相關；因為雨水會導致花朵凋謝。\n野生藍莓 vs 人工種植藍莓 # 經過資料查詢後，我們知道市場上的藍莓可以大致上分為野生藍莓與人工種植的藍莓，而他們之間有什麼不同呢？\n通常人工種植的藍莓具有更高的產量，這是因為人工種植的藍莓可以在受控的環境中進行管理，包括土壤、水分、陽光、溫度等因素都可以被精確地調節，從而最大程度地促進藍莓的生長和產量。\n此外，人工種植的藍莓往往使用高產量的品種，並且可以利用現代農業技術和機械化設備來提高生產效率，進一步增加產量。相比之下，野生藍莓的產量通常較低，因為它們生長在自然環境中，受到自然條件的限制，例如土壤品質、水分供應、陽光照射等因素都無法被控制。此外，野生藍莓的生長可能受到天敵和自然災害的影響，進一步影響了產量。\n我們將降雨天數除以平均降雨天數，大致從 Figure 6: Berry Species 將藍莓生長的天數分為兩種，I型藍莓（生長周期較短）的產量較高，我們猜測可能是人工種植，而II型藍莓（生長周期較長）的產量較低，我們猜測可能是野生藍莓。\n那麼我們是如何區分它們的呢？我們在 Figure 5: Grandma Berry? 中，刪除了一些單個數據，不只是因為他們相較於兩種藍莓的週期而言差距比例甚大，有些數據更是十分的不合理，請試想一下，生長週期是其他藍莓50倍長的藍莓，或許可以稱為「藍莓奶奶」了吧。在我們調整完數據之後，總共還有20,000多條數據。\n生長環境對藍莓的影響 # 接著我們想透過觀察產區的均溫，來尋找關於生長區域所帶來的特徵。我們將最高上限氣溫和最高下限氣溫相加並取平均值，得到一個新的平均溫度，我們使用新的平均溫度將產區大致分為四個區域，見Figure 7: Four Areas 。\n在繪製了區域和產量比較圖後，由於藍莓實際上也有適合生長的溫度，過高或過低的溫度都會抑制其生長。我們發現儘管第2區和第3區的溫度不是最高的，從 Figure 8: Compare Areas 中可以看出，它們的產量在四個區域中是最好的。\nMethod # K-Fold Cross Validation # 在開始模型建構之前，我們使用交叉驗證來確保模型不會過度擬合。我們採用K折交叉驗證的方法，在這裡我們簡單的做一個介紹。\nK 折交叉驗證將數據集分成 K 個相似大小的子集，稱為 folds，然後進行K次訓練和測試。在每一次迭代中，將其中一個 fold 作為測試集，其餘的 K-1 個 fold 作為訓練集。而在 K 次迭代完成後，將每次測試的性能評估指標取平均值，作為模型的最終性能評估。\n交叉驗證可以提供對模型性能的更穩健的估計，因為它在不同的子集上進行了多次測試和訓練，減少了對單一切分的依賴性，也避免了極端的樣本組合可能對模型 over fitting 的影響。在這裡的模型建構中，我們將 K 設定為 5 。\nCompare Models # 我們總共對四種模型進行了個別的調整。本次比賽的參賽人數約 1900 人，而評分標準基於模型的平均誤差（MAE）。最後調整後的線性模型排名約為 1570，隨機森林排名為 1400，XGBoost 排名較低，在 1030 左右，表現最好的是 LightGBM，排名 334。\n那麼我們是如何做到的呢，資料清洗、特徵工程與對模型的調整是必要的，接下來我們會對資料的建模與調整做更近一步的介紹。\n這是一個被設計好的問題？ # 在進入模型介紹之前，透過觀察產量的變數後，我們懷疑它是一個看起來像類別的數值變數。但我們最終將其視為連續變數。因為如果我們將這個問題視為分類問題的話，會有約500個類別，甚至有的類別只有一個樣本，基本上無法將變數視為離散的來進行預測。\n而我們也發現了一些資料中的不合理之處，例如變數中的 Honeybees 在標準化之後，我們發現甚至有密度超過46個標準差的蜜蜂，看起來十分可疑。考慮到這個變數與其他變數之間交互作用均對模型幾乎沒有影響，經過思考後我們決定刪除 Honeybees。\n但這只是一個開頭，綜合前面在整理資料時發現諸如「藍莓奶奶」之類的問題後，不禁讓我們對數據對數據的紀錄有點懷疑，甚至思考這些是否都是被刻意混入的錯誤資訊。\n而 Kaggle 官方的這筆資料是使用小數據產生大數據來訓練，但是如果仔細查看數據，會發現有些數據是相似更甚至是大致相同的，因此在完成模型建構後，我們懷疑測試集也是從訓練集中分離出來的。\nLinear Regression # 線性模型可以很好地表達我們在數據中發現的問題。從 Figure 9: Linear Model 中可以看到，資料實際上其實具有離散型變量的特徵，但我們對此並沒有太多辦法。\n就像在前面所提到，我們觀察出產量是個看起來具有類別特徵的數值變量一樣，模型也可能會獲取這種信息，因此當模型輸出預測結果時，也會傾向於預測類別，但一旦分類不正確，就會導致非常大的錯誤。這個問題在線性模型中特別嚴重，後續模型數據中段的預測性能將比線性模型好很多，但在兩端點仍會有類似問題。我們目前可能無法很好地解決這類問題。\n回到比較模型優劣的部分，線性模型在這些數據中的優勢是運行速度快且模型從統計角度高度可解釋，同時可以快速篩選出顯著影響模型的異常資料。缺點是模型的結果受異常值的高度影響，而且依賴於數據的線性關係，導致我們需要將資料清洗過才能有效的做呈現。\n由於後續模型的結果的分佈圖與線性模型相似，因此僅展示圖9，不再展示更多結果。\nRandom Forest # 關於隨機森林模型我們並沒有太多著墨於此，由於調整模型的效果不大，甚至更換種子的隨機性影響都大於模型本身的參數調整。\n在這筆資料中，使用它來建模的優點是它是開箱即用的，基本上不需任何的調整表現就比線性模型更佳，我想這可能是資料型態本身較適合 tree base 的模型，由於看似多分類問題的數據型態導致他可能並不是那麼適合以線性的方法來建模，不過他並沒有改善我們在線性模型中所遇到的問題，因此這個模型對我們結果的提升十分有限。\nXGBoost # 當我們開始嘗試使用 XGBoost 建模調參時，我們從中發現了一件重要的事，甚至使我們對模型的調整方向有很大的轉變。起初我的想法源於參考原始論文後，樹深不宜太深所以均往其他參數方向做調整，但經過一系列測試後，我們將樹的深度設置為5，如此的深度超出了我的預期。\n基於我之前在模型調優方面的經驗，如此深的樹可能會導致過擬合。但在這裡卻能使模型的表現更好，我假設更深的樹所帶來過擬合造成結果恰好會如同我們前面所看到的數據分佈一般，即是具有類別特徵的連續型變數作為結果。\n同時這個結果讓我思考了一個問題：訓練集和測試集是否來自相同的底層過程？是否因為這樣的原因而導致我們原以為的過凝合其實是更優的解決方案，即我們認為會產生的過凝合問題在這筆資料中並不存在。無論如何，在這個數據集中，更深的樹深確實提高了預測準確性，並且對資料兩端值時的預測值是四個模型中較好的。\nLightGBM # 接著延續我們在 XGBoost 中得到的想法，在 LightGBM 的模型調整中，我們採用了生成更多葉節點、增加迭代次數以及使用較低學習率的策略。最終我們將葉節點數量設置為28。\n然而，我們觀察到這些葉節點往往集中在特定的分支上，導致子模型甚至比 XGBoost 中的樹深要更深。這也超出了我對模型的預期。與我們在 XGBoost 中的發現類似，這是否意味著更複雜的樹增強了模型的準確性？進而使 LightGBM 成為我們目前解果最好的單一模型。但 LightGBM 雖然整體精度是四個模型中最高的，但它犧牲了兩端數據預測的準確度。\nConclusion # 經過大量時間和精力的投入，通過特徵工程、調整參數和改進模型擬合度，我們發現研究結果仍然相當出色，且模型的排名表現也十分優異。在這次的分析中我們有三個主要結論。\n首先是數據清洗與特徵工程的重要性，在清洗過程中可以挑出異常數據。否則，未經清洗的數據直接投入模型，效果通常不會很好。特徵工程經過用了解藍莓的種類和生產區域後，所生成的特徵對後續分析有很大幫助。\n第二是調整參數和選擇模型也非常重要。通過調整不同的參數可以提高模型的適應性，而不同模型的結果也，線性模型解釋力強，但效果不佳；隨機森林和 XGBoost 解釋力不強，但對數據兩端極值的預測值較為合理；LightGBM 雖然整體準確率最高，但犧牲了兩端的數據。如果將這這些模型結合成一個混合模型，排名分數應該會進一步提高。\n最後是關於後面模型的參數調整讓我獲益良多，有種讀萬卷書不如行萬里路的感覺，在完成這次資料分析前，我不曾試過這樣進行模型調整效果反而會更好的案例，也對資料樣貌有了更深一層的認知。\n後續工作與感想\n關於這筆資料，或許可以將更多異常數據提出來，形成一個新的數據集並單獨建模，這應該對提高準確性有很大幫助。也可以嘗試混合模型，但需要對混合模型的架構進行更多嘗試以求獲得更好的解果。總而言之，我們在 Kaggle Playground 競賽中學到了很多，這次分析對我而言是一次很好的嘗試。也謝謝閱讀至此的各位，我會繼續在數據分析上力求精進，以求能做出更好的成果。\n","date":"5 June 2023","externalUrl":null,"permalink":"/projects/kaggle_s3e14/","section":"Projects","summary":"利用從 Kaggle 競賽中提供的野生藍莓產量預測數據，開發多個基於不同機器學習方法，用於估算野生藍莓產量的預測模型","title":"藍莓產量預測報告","type":"projects"},{"content":"欣賞電影與劇集是現代人眾多生活消遣之一，而如何推薦一部令使用者滿意的影集也成為各個應用程式在現代科技發達下的新戰場。我們生活中處處皆充滿了推薦系統的影子，舉凡購物網站、社群媒體、甚至連搜尋引擎均是如此，而我基於對這個領域的好奇心，加上在碩士論文研究上有考慮向推薦系統發展的緣故，這個學期我跨系修習了資工系的推薦系統課程，因此也就衍生出了這份期末報告，由於教授課程規劃的緣故，所有的報告統一均採用 MovieLens 100k 這筆資料，而這次的報告也在課程中拿到最好的成績。\nInduction # 這筆資料來自於 grouplens 這個網站，資料中一共有 100,000 條評分，它們分別来自 943 位使用者對 1682 部電影的評價。而本次的資料一共分成三個部分，它們分別有使用者的相關數據、電影的相關數據以及使用者對電影的評分。接著就讓我們來一一介紹我們有使用到的部分。\n使用者數據：\nuser_id\nage : 使用者的年齡\ngender\noccupation : 使用者的職業\n電影數據：\nmovie_id\nmovie_title\nrelease_date : 電影上映日期\nunknown ~ Western : 這裡一共有 19 個電影類別，均為 0 1 的類別變數，用來表示這部電影是否屬於該類別。\n使用者對電影的評分：\nuser_id\nmovie_id\nrating\ntinestamp : 使用者對電影的評分時間\n我們的最終目標是利用這筆資料的各個部分進行電影推薦系統的建模。\nExploratory Data Analysis, EDA # Movies and User Data # 由於在使用者的數據上我們並沒有太多的發現，這是關於使用者圖表的連結 ，我們這裡的分析會以電影資料為主。\n首先我們對電影的上市日期做了初步的分析，透過文字處理將資料切割成年份及日期後，我們從中發現在數據中，電影的發布年份在 1993 年之後呈現爆發式增長，經查證後發現在當年大該發生了這些事件：\n《侏羅紀公園》的電腦生成圖像 (CGI)：這部電影由史蒂芬·斯皮爾伯格執導，標誌著 CGI 技術在電影中的重大飛躍。《侏羅紀公園》使用了創新的電腦生成技術來創造真實的恐龍，這在當時是前所未有的。這不僅推動了視覺特效的極限，也改變了未來電影製作的方式。\n數字音效和 Dolby SR-D 音效系統：1993 年也見證了音效技術的進步。Dolby SR-D（現在稱為Dolby Digital）是首次在商業電影院提供的數字音效系統，首次應用在《侏羅紀公園》中，提供更清晰、更動態的聲音體驗。\n柯達的 Cineon 數位影像系統：該系統於 1993 年推向商業市場，對電影行業的數字後期製作領域產生了重要的影響。Cineon 系統是一種先進的數位影像處理系統，它允許用戶對攝影底片進行高質量的掃描、處理、和輸出，從而實現數字化的非線性編輯和色彩校正。這一系統的推出標誌著從傳統的底片處理向數位工作流程的重要轉變。\n我們猜測是由於電影製作成本與技術門檻的下降及侏羅紀公園上映後大眾對電影燃起了興趣，進而使電影產業如雨後春筍般產出許多運用新技術的作品。\n但是當我們進一步剖析資料後發現，電影的上映日期上具有很大的問題，竟然有超過六成的電影是在一月一日上映的，這顯然不合理，我們因此並沒有對日期內的資訊做進一步的挖掘。\n接著我們先關注到的是電影的類別，我們好奇這個離散且稀疏的矩陣中含有哪些隱藏的資訊，讓我們先來關注每個類別分別有多少電影。我們可以從圖中看出劇集、喜劇、動作、驚悚與浪漫依序為最多的前五類。\n由於每個電影的類別不只擁有一類的緣故，因此我們決定觀察一下每部電影它們類別的數量分佈來做進一步的了解，而我們從中發現約有百分之八十的資料具有一至兩個類別，而擁有三個以上類別的資料是十分稀少的，由於想要視覺化它必須先進一步處理，因此我們先將剩下的發現透過 Jaccard 與 PCA 的方式做進一步解析。\nJaccard # 這裡我們簡單介紹一下 Jaccard 這個方法，它是用於計算兩個集合 $U$ 和 $V$ 的相似度，公式如下所示。\n\\( J(U,V) = \\frac{|U \\cap V|}{|U \\cup V|} \\)\n為了能更淺顯易懂的解釋它，讓我來舉一個簡單的例子：\n今天有 \\( U \\) 和 \\( V \\) 兩名顧客進入賣場買東西，我們想知道他們兩人所消費的物品相似度有多高。接著我們假設 \\( U \\) 買了礦泉水和洋芋片，而 \\( V \\) 買了汽水及洋芋片，那麼他們兩人的交集 \\( U \\cap V \\) 只有洋芋片一項物品，而他們的聯集 \\( U \\cup V \\) 有礦泉水、汽水和洋芋片，那麼他們兩人的 Jaccard 相似度便是 \\( \\frac{1}{3} \\)。同理可知若是兩個人買了完全一樣的物品，那麼他們的相似度便會是 1，反之，若兩人沒有購買任何相同的物品，那麼他們的相似度便會是 0。\n接著我們再來講解一下與相似度相同的一個概念，它便是 Jaccard 距離。\n\\( d_J(U,V) = 1 - \\frac{|U \\cap V|}{|U \\cup V|} \\)\nJaccard 距離便是將原本相似度的概念翻轉過來，若是兩個人買了完全一樣的物品，那麼他們的距離便會是 0，反之，若兩人沒有購買任何相同的物品，那麼他們的距離便會是 1。\n下面讓我們來看看原本離散的矩陣透過 Jaccard 轉置後所得到的，下面分別是原始的稀疏矩陣及以電影類別分類轉置後的距離矩陣，透過 Jaccard 算法可以讓我們在後續進行數據將維及視覺化的效果有更近一步的提升。\n至於主成分分析（PCA）和 t 分佈隨機鄰近嵌入（t-SNE）兩個算法，由於這裡的篇幅有限就不詳細說明，後續我會新增關於這兩個算法的詳細解說。\nDimension Reduction # 接著讓我們來用圖片來展示將電影類別資料視覺化後的成果，又因為在三維空間能更好的說明它，因此我也製作了可互動式的網頁連結 ，裡面分別有原始的程式檔及可以進行縮放及旋轉的圖片，以便於搭配文字說明可以更好的理解內容。\n首先我們先來看到第一張圖中左上角的三個點，它們分別是浪漫、喜劇及劇集，這三種類型的電影相較於其他類型較為互相接近，它們的距離可以被我們解釋為較為相似的主題，而第一張圖中間靠右的三個點他們分別是動作、冒險及科幻，\n而下方的群體便需要我們轉動角度才能更好的觀察它們，我們可以在第二張圖下方看到的三個點分別是音樂、卡通及兒童。\n最後我們可以將剩下的點大致上看成三個區塊，他們分別是驚悚與恐怖片、西部與紀錄片、神秘黑暗的犯罪電影。\n從上面的圖我們可以很好的釐清電影類別之間的關係，同時他們的距離關係也可以被合理的解釋。但因為這次報告時間的緣故，我們並沒有將這個視覺化成果的資訊加入後續的模型中，但這對我而言是一個對數據處理新的嘗試，同時也在分類上做出了不錯的成效。\nRecommender System # 在開始構築推薦系統前，我們先對資料進行了刪減，原因是我們認為，要將該條評分用於構建推薦系統需要滿足以下兩個條件：\n該名評分的使用者必須要曾評分過足夠多的電影，那麼他的評分相對只評過一兩部電影的人而言，分數會較為具有參考性，因為它的評分是建立在一定的樣本基數上的。\n電影同樣也須收到足夠的評分才會被用於訓練模型。\n經過對不同組參數的設定比較，我們最終決定採用曾評分過 50 則評分的使用者與曾收到 50 則評分的電影作為訓練資料。而經過我們的篩選，評分剩下 73544條，參與的使用者與電影也從 943 位使用者對 1682 部電影的評價變成 568 位使用者對 603 部電影的評價。\n接著我們定義模型訓練時，用戶必須給出從1-5分的範圍中給出4分才算是對於這部電影的正面評價，同時我們採用K折交叉驗證（K-Fold Cross Validation）來確保模型不會過度擬合，在這裡的模型建構中，我們將K設定為5。接下拉讓我們來簡單的介紹一下這次分析有使用到的方法。同時原始程式與推薦出的結果我將它放在下方不同推薦系統標題中的連結。\nUser-Based Collaborative Filtering # 首先是用戶為基礎的協同過濾，又簡稱為（User-Based CF）是一種基於用戶相似度的推薦方法。該方法假設如果兩個用戶對相似的物品評分相似，那麼他們在其他物品上的評分也可能相似。目標為通過找出與目標用戶評分行為相似的用戶，向目標用戶推薦這些相似用戶喜歡的物品。\n他的優點為容易理解和實現，並且當有大量用戶評分數據時，推薦效果較好。但他也有隨著用戶數量增長，計算相似度的開銷將會變得很大導致效率低下的問題，同時也會面臨冷啟動問題，也就是當有新物品進入推薦系統時，由於資料量不足，系統無法第一時間抓取足夠的數據以在空間中定義相似度的情況。\nItem-Based Collaborative Filtering # 接著是以物品為基礎的協同過濾（Item-Based CF），是基於物品相似度的推薦方法。該方法假設如果一個用戶喜歡某個物品，他可能會喜歡與之相似的物品。因此我們通過找出與目標物品相似的物品，進而可以向用戶推薦這些相似物品。\n優點為當物品數量相對穩定時，計算效率較高。並且在用戶的冷啟動問題相較而言較容易處理。不過當物品數量巨大時，計算相似度的開銷仍然很大，同時依然面臨在推薦新物品時的冷啟動問題。\nFunkSVD # 最後則是 FunkSVD，它是一種基於矩陣分解的推薦算法，利用奇異值分解（Singular Value Decomposition，SVD）來分解用戶與物品的評分矩陣，試圖將用戶和物品映射到一個共同的潛在特徵空間，並基於這些潛在由線性組合生成的特徵進行推薦。\n它的優點為能夠捕捉到用戶和物品的隱含特徵，並且對於稀疏矩陣表現優秀又能夠處理大規模數據。但是訓練過程較慢，尤其是當數據量很大時更是如此。\n我們可以在下方的 ROC曲線 看到不同推薦系統在推薦不同數量電影時的表現，可以看出FunkSVD作出的推薦相較其他模型更為精準，但其他兩個模型也相較隨機推薦的效果來的佳，作為構建成本較低的方案也不失為一個選擇。\n最後為了能更直觀的看出各個推薦系統之間的效果，我們也繪製了比較模型與真實評分誤差的圖表以供參考。\nConclusion # 我們在這次的資料分析中嘗試了不同類型目標的模型建構，透過我們建構的模型可以看出不同方法在這次的電影資料中具有不同的效果，而這些方法所適合的資料類型也不盡相同，但他們都在這次的建模中發揮不錯的成效。同時我們也將電影的類別透過 PCA 求得特徵向量後在視覺化做呈現，這讓我們更加釐清電影之間的關係，也為後續 FunkSVD 的模型建構做先一步的數據探索。\n後續工作與感想\n關於這筆資料我們或許可以將其做成 ShinyApps 將其連結至網路做更好的結果展示，同時也能嘗試將前面所提取出的特徵進一步優化模型的效果。而最後我對這次報告的經驗讓我於推薦系統領域獲益良多，讓我得以繼續在資料分析領域尋找許多不同的問題去細細品味。\n","date":"21 December 2022","externalUrl":null,"permalink":"/projects/r_recommender/","section":"Projects","summary":"運用電影評分資料資料嘗試構建基於不同方法的推薦系統","title":"MovieLens 100k 推薦系統","type":"projects"},{"content":"","date":"11 March 2022","externalUrl":null,"permalink":"/categories/mathematics/","section":"Categories","summary":"","title":"Mathematics","type":"categories"},{"content":"","date":"11 March 2022","externalUrl":null,"permalink":"/tags/sampling/","section":"Tags","summary":"","title":"Sampling","type":"tags"},{"content":"","date":"11 March 2022","externalUrl":null,"permalink":"/tags/simulation/","section":"Tags","summary":"","title":"Simulation","type":"tags"},{"content":" Question # 我們想要以Ｒ語言實作拒絕抽樣的模擬實作，在這裡我們會以蒙地卡羅的方法來將目標函數以覆蓋接受域的作法來實現它。\nMethod # 首先我們需要給訂一個目標函數 f，同時給定一個生成範圍 [a, b]，由於我們想要使用 uniform(0, 1) 來生成符合 f 在 [a, b] 區段機率密度的樣本，因此我們同時需要給定 uniform 的機率密度函數，而α 是一個常數，用於調整拒絕機率，確保生成的樣本符合目標機率密度函數。\n對於每個生成的 x 值，計算了對應的機率密度比率 test_x，然後與在 [0, 1] 上生成的隨機數 test_u 進行比較。如果 test_u 小於等於 test_x，則接受該樣本，否則拒絕。\nrejection_sampling \u0026lt;- function(f, g, alpha, a, b, n){ total_n \u0026lt;- 0 accept_x \u0026lt;- c() while(length(accept_x) \u0026lt; n){ total_n \u0026lt;- total_n + 1 x \u0026lt;- runif(1, a, b) test_x \u0026lt;- f(x) / (alpha * g(a, b)) test_u \u0026lt;- runif(1, 0, 1) if (test_u \u0026lt;= test_x){ accept_x \u0026lt;- c(accept_x, x) } } return(list(c(length(accept_x), total_n), accept_x)) } Result # 下面我們給定 f 在 [-1, 1]區間，在α = pi的情況下生成 10000 筆模擬資料並繪製結果，紅線是我們的目標函數f，而灰色部分是生成資料的 Histogram。\nf \u0026lt;- function(x) pi/2*(sqrt(1-x^2)) unif_pdf \u0026lt;- function(a, b) 1/(b-a) alpha \u0026lt;- pi sampling \u0026lt;- rejection_sampling(f, unif_pdf, alpha, -1, 1, 10000) area \u0026lt;- integrate(f, -1, 1)$value hist(sampling[[2]], breaks = 15, probability = TRUE, xlim = c(-1, 1), ylim = c(0, 0.8), xlab = \u0026#34;x\u0026#34;, main = paste0(\u0026#34;Histogram of Sampling (alpha = \u0026#34; ,round(alpha, 2) ,\u0026#34;)\u0026#34;)) curve(f(x) / area, from = -1, to = 1, col = \u0026#34;red\u0026#34;, lwd = 2, add = TRUE) Compare α # 最後讓我們來比較在不同 α 的情況下所需迭代次數的變化，可以在下圖中看出在 α = pi 之後所需迭代次數與 α 成正比。\nalpha_vector \u0026lt;- c(1, pi/2, pi, 2*pi, 4*pi) df \u0026lt;- data.frame() for(alpha in alpha_vector){ sampling \u0026lt;- rejection_sampling(f, unif_pdf, alpha, -1, 1, 10000)[[1]] df \u0026lt;- rbind(df, c(alpha, sampling[2])) } colnames(df) \u0026lt;- c(\u0026#34;alpha\u0026#34;, \u0026#34;iterations\u0026#34;) plot(df$alpha, df$iterations, type = \u0026#34;b\u0026#34;, pch = 19, col = \u0026#34;blue\u0026#34;, xlab = \u0026#34;Alpha\u0026#34;, ylab = \u0026#34;Iterations\u0026#34;, main = \u0026#34;Iterations vs Alpha\u0026#34;) 但是在 α \u0026lt; pi 的情況下會導致生成的機率密度無法涵蓋目標函數。這裡的原因是因為計算f(x) / g(a, b)後我們會得到[-0.5, 0.5]這個區間的結果，而函數 f 的最大值為 pi/2，因此如果 α \u0026lt; pi 會導致在函數的最大值部分沒辦法被接受，而因為中央的樣本缺乏導致生成的機率密度較原本而言中間較低兩側較高的情況。\n","date":"11 March 2022","externalUrl":null,"permalink":"/posts/r_rejection_sampling/","section":"Posts","summary":"以蒙地卡羅法來將目標函數以覆蓋接受域來實現拒絕抽樣","title":"拒絕抽樣實作","type":"posts"},{"content":"這裏我們將使用黎曼積分法、梯形積分法、辛普森積分法及蒙地卡羅積分法來實作他，下面是他們分別簡單的介紹。\n黎曼積分法：將給定區間等分成 n 條小區間，然後計算每個小區間的函數值乘以小區間的寬度，再將這些乘積相加得到積分值。 梯形積分法：將給定區間等分成 n 條小區間，然後計算每個相鄰小區間的函數值乘以相鄰小區間的平均寬度，再將這些乘積相加得到積分值。 辛普森積分法：將給定區間等分成 n 條小區間，然後使用三點法，將相鄰小區間的函數值組合成一個曲線，再將這些曲線下面積的估計值相加得到積分值。 蒙特卡羅積分法：通過在給定區間內生成隨機數，計算這些隨機點對應函數值的平均，再乘以區間寬度，得到積分值的估計。 my_integrate \u0026lt;- function(Fun, lower, upper, n=10, digit=15, method=\u0026#34;rect\u0026#34;){ options(digits = digit) if(method == \u0026#34;rect\u0026#34;){ sum(sapply(1:n, function(x, Fun) Fun(x*(upper-lower)/n+lower)*(1/n), Fun)) }else if(method == \u0026#34;tri\u0026#34;){ sum(sapply(1:n, function(x, fx, dx) (fx[x]+fx[x+1])*dx/2, sapply(seq(lower, upper, (upper-lower)/n), Fun), (upper-lower)/n)) }else if(method == \u0026#34;simp\u0026#34;){ dx \u0026lt;- (upper-lower)/n; x \u0026lt;- seq(lower, upper, dx) odd \u0026lt;- seq(length(x)-2, 2, by=-2); even \u0026lt;- seq(length(x)-1, 2, by=-2) sum(c(Fun(x[1]), Fun(x[n+1]), sapply(x[odd], function(x, Fun) 2*Fun(x), Fun), sapply(x[even], function(x, Fun) 4*Fun(x), Fun)))*dx/3 }else if(method == \u0026#34;mtca\u0026#34;){ mean(Fun(runif(n, min = lower, max = upper)))*(upper-lower) } } 下面我們分別將他們的積分結果、誤差及函數收斂的速度存儲在一個 data.frame 中。通過這些結果，可以比較不同方法在不同情況下的性能表現。我們可以從中看出，梯形法相較黎曼和的收斂速度提升了兩倍，而辛普森更是梯形法收斂速度的四倍。\nI \u0026lt;- 0.746824132812427 # Curect Anwser f \u0026lt;- function(x) exp(-x^2) # Target Function Inputs \u0026lt;- sapply(2:9, function(x) 2**x) # Given Inputs from 4 to 512 Methods \u0026lt;- c(\u0026#34;rect\u0026#34;, \u0026#34;tri\u0026#34;, \u0026#34;simp\u0026#34;, \u0026#34;mtca\u0026#34;) # Integrate Methods result_df \u0026lt;- data.frame() # I save all results by empty Dataframe first \u0026lt;- TRUE # We can\u0026#39;t cbind empty dataframe in first loop, so I set this for(Method in Methods){ # loop by Methods results \u0026lt;- sapply(Inputs, function(x) my_integrate(f, 0, 1, x, 15, Method)) if(first==FALSE){ result_df \u0026lt;- cbind(result_df, results) }else{ result_df \u0026lt;- results first \u0026lt;- FALSE } } rownames(result_df) \u0026lt;- Inputs colnames(result_df) \u0026lt;- Methods error_df \u0026lt;- result_df-I # error for results ratio_df \u0026lt;- t(sapply(1:(length(Inputs)-1), # ratio for results function(x) error_df[x+1,]/error_df[x,])) rm(f, first, I, Inputs, Method, Methods, results) ","date":"25 February 2022","externalUrl":null,"permalink":"/posts/r_intergate/","section":"Posts","summary":"使用黎曼積分法、梯形積分法、辛普森積分法及蒙地卡羅積分法來模擬程式計算積分的過程","title":"多種積分函數實作","type":"posts"}]